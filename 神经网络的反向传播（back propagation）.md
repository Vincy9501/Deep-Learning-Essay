 
前面学到过[[神经网络]]的基础知识，以及它是如何传递信息的，经典例子是手写数字识别，数字的像素值被输入到网络第一层的784个神经元里。

从[[梯度下降算法（Gradient Descent）]]一章，我们了解到：所谓学习就是指，我们要找到特定的权重偏置从而使一个成本函数最小化。

对于手写数字识别的例子，计算一个训练样本的cost，需要求出网络的输出与期待的输出之间每一项的差的平方和，然后对于成千上万个训练样本都这么算一遍最后取平均，这就得到了整个网络的cost。复杂点说我们要求的是成本函数的负梯度，它告诉你如何改变所有连线上的权重偏置才好让cost下降得最快。

反向传播正是利用这个求梯度的。

梯度向量每一项的大小是在告诉大家，成本函数对于每个参数有多敏感。

# 1. 介绍

反向传播是一种监督学习的方法，神经网络用来更新参数，使网络的预测更加准确。参数优化过程是使用称为**梯度下降**的优化算法实现的（这个概念在您继续阅读时会非常清楚）。

## 1.1 直观了解

因为成本函数涉及到对成干上万个训练样本的cost取平均值，所以我们调整每一步梯度下降用的权重偏置，也会基于所有的训练样本。但为了计算效率之后咱们会讨个巧，从而不必每一步都非得要计算所有的训练样本。

目前我们只关注一个训练样本，这一个训练样本会对调整权重和偏置造成怎样的影响呢?

假设神经网络还没有训练好，我们输入2，想要获得2，就要使输出层的2这个神经元的激活值变大。前面提到，这个激活值等于把前一层所有激活值的加权和加上一个偏置，再通过sigmoid ReLU之类的挤压函数最后算出来的。 ^0c0932

![[Pasted image 20230201115736.png]]

所以我们有三个选项：
- 增加偏置
- 增加权重
- 改变上层激活值

**调整权重**

各个权重它们的影响力各不相同，连接前一层最亮的神经元的权重影响力也最大，因为这些权重会与大的激活值相乘。所以对于这个训练样本，增大了这几个权重值对最终成本函数造成的影响就比增大连接黯淡神经元的权重所造成的影响要大很多。

所以：**调整连接激活值大的神经元的权重**。

**改变上层激活值**

如果所有正权重连接的神经元更亮，所有负权重连接的神经元更暗的话，那么数字2的神经元就会更强烈地激发。和改权重的时候类似，我们想造成更大的影响，就要依据对应权重的大小对激活值做出呈比例的改变。

**我们还需要最后一层其余的神经元的激发变弱。但是其余每个输出神经元，对于如何改变倒数第二层都有各自的想法。

所以我们会把**数字2神经元的期待和别的输出神经元的期待全部加起来**作为对如何改变到数第二层神经元的指示。这些期待变化不仅是对应的权重的倍数，也是每个神经元激活值改变量的倍数。

我们把所有期待加起来，就得到了一串对倒数第二层改动的变化量。有了这个就可以重复这个过程，改变影响倒数第二层神经元激活值的相关参数。从后一层到前一层把这个过程一直循环到第一层。

之后，我们要对其他所有的训练样本同样地反向传播，记录下每个样本想怎样修改权重与偏置，最后**再取一个平均值**。这里一系列的权重偏置的平均微调大小，就是cost函数的负梯度。


## 1.2 随机梯度下降

如果每一个激活值都用上每一个训练样本来计算的话那么花的时间就太长了，所以一般我们会这么做：首先把训练样本打乱，然后分成很多组minibatch，然后你算出这个minibatch下降的一步。这不是成本函数真正的梯度，然而每个minibatch都会给你一个不错的近似值。

## 1.3 小结

- 反向传播算法算的是**单个训练样本**想**怎样修改权重与偏置**，不仅是说每个参数应该变大还是变小，还包括了这些变化的比例是多大才能最快地降低cost。
- 选择随机梯度下降，将样本分为多组minibatch，每个minibatch算出下降的一步，迭代，最终你就会收敛到成本函数的一个局部最小值上。

# 2. 微积分实现

## 2.1 例子

从最简单的部分开始：
![[Pasted image 20230201122944.png]]
图上这个网络就是由3个权重和3个偏置决定的，我们的目标是**理解成本函数对于这些变量有多敏感**，这样我们就知道**如何调整函数才可以使得cost降低得最快**。

![[Pasted image 20230201124603.png]]

我们先来关注最后两个神经元吧，给最后个神经元的激活值一个L上标表示它处在第L层。给定一个训练样本，我们把这个最终层激活值要接近的目标叫做y。所以y要么是0要么是1。

最终层的激活值有如下公式：
$$a^{(L)} = σ(w^{(L)}a^{(L-1)} + b^{(L)})$$

将加权值用z表示：
$$z^{(L)} = w^{(L)}a^{(L-1)} + b^{(L)}$$
$$a^{(L)} = σ(z^{(L)})$$
概括起来：我们拿权重、前一个激活值、偏置值算出z再算出a，再用上y算出cost。
![[Pasted image 20230201125110.png]]

我们可以想象每个数字都对应一个数轴，第一个目标是理解成本函数对权重$w^{(L)}$的微小变化有多敏感。

![[Pasted image 20230201125245.png|275]]
或者换句话说，求C对$w^{(L)}$的导数，然后把$\partial C$当作改变w对C值造成的变化。
所以我们求的是：
$$\frac{\partial C_0 }{\partial{w^{(L)}}}$$
概念上说，$w^{(L)}$的微小变化会导致$z^{(L)}$产生变化，然后会导致$a^{(L)}$产生变化，最后影响到cost值。

我们把式子拆开，首先求
$$\frac{\partial z^{(L)} }{\partial{w^{(L)}}}$$
然后考虑$a^{(L)}$的变化量
$$\frac{\partial a^{(L)} }{\partial{z^{(L)}}}$$
以及最终$C_0$的变化量比上直接改动$a^{(L)}$产生的变化量
$$\frac{\partial C_0}{\partial{a^{(L)}}}$$
这就是链式法则，把三个比相乘就可以算出C对$w^{(L)}$的微小变化有多敏感。
$$\frac{\partial C_0 }{\partial{w^{(L)}}} = \frac{\partial z^{(L)} }{\partial{w^{(L)}}} \frac{\partial a^{(L)} }{\partial{z^{(L)}}} \frac{\partial C_0}{\partial{a^{(L)}}}$$

进一步：
$$\frac{\partial C_0}{\partial{a^{(L)}}} = 2(a^{(L)} - y)$$
这意味着导数的大小跟**网络最终输出**减**目标结果**的差成正比。如果网络的输出差别很大，即使w稍稍变点cost也会改变非常大。
$$\frac{\partial a^{(L)} }{\partial{z^{(L)}}} = \sigma ^{'} (z^{(L)})$$
$$\frac{\partial z^{(L)} }{\partial{w^{(L)}}} = a^{(L - 1)}$$
所以**权重对最后一层的加权和影响有多大取决于前一层的神经元**。这就是为什么一同激活的神经元关联在一起。

最后得到：
$$\frac{\partial C_0 }{\partial{w^{(L)}}} = \frac{\partial z^{(L)} }{\partial{w^{(L)}}} \frac{\partial a^{(L)} }{\partial{z^{(L)}}} \frac{\partial C_0}{\partial{a^{(L)}}} = a^{(L - 1)} \sigma ^{'} (z^{(L)}) 2(a^{(L)} - y)$$
不过这只包含了一个训练样本的cost对$w^{(L)}$的导数，由于总的成本函数是许许多多训练样本所有cost的总平均，它对$w^{(L)}$的导数就需要求这个表达式对每个训练样本的平均：
$$\frac{\partial C }{\partial{w^{(L)}}} = \frac{1}{n} \sum^{n-1} _{k=0}\frac{\partial C_k }{\partial{w^{(L)}}}$$
梯度向量$\nabla C$本身则由成本函数对每一个权重和每一个偏置求偏导构成：
![[Pasted image 20230201132825.png]]

同理对b求偏导可以得到：
$$\frac{\partial C_0 }{\partial{b^{(L)}}} = \frac{\partial z^{(L)} }{\partial{b^{(L)}}} \frac{\partial a^{(L)} }{\partial{z^{(L)}}} \frac{\partial C_0}{\partial{a^{(L)}}} =\sigma ^{'} (z^{(L)}) 2(a^{(L)} - y)$$

这里也涉及了反向传播，**这个成本函数对上一层激活值的敏感度是**：
$$\frac{\partial C_0 }{\partial{a^{(L-1)}}} = \frac{\partial z^{(L)} }{\partial{a^{(L-1)}}} \frac{\partial a^{(L)} }{\partial{z^{(L)}}} \frac{\partial C_0}{\partial{a^{(L)}}} =w^{(L)}\sigma ^{'} (z^{(L)}) 2(a^{(L)} - y)$$

![[Pasted image 20230201133215.png]]
我们虽然不能直接改变激活值，但是我们很有必要关注这个值，因为我们可以反向应用链式法则来计算成本函数对之前的权重和偏置的敏感度。

再复杂一点：


我们用加上下标的神经元来表示L层的若干个神经元，用k来标注(L-1)层的神经元，j则是L层的神经元。要求成本函数，我们从期望的输出着手，**计算上一层激活值和期望输出的差值的平方然后求和**：
![[Pasted image 20230201134303.png]]
我们把$a^{(L-1)}_k$和$a^{(L)}_j$的权重记为$w^{(L)}_{jk}$，有：
![[Pasted image 20230201134626.png]]
链式法则形式的导数表达式所描述的成本对某个权重的敏感度也是一样的：
![[Pasted image 20230201135840.png|475]]
唯一改变的是成本对(L-1)层激活值的导数：
![[Pasted image 20230201140011.png]]
此时激活值可以通过不同的途径影响成本函数，就是说神经元一边通过$a_0^{(L)}$影响成本函数，一边通过$a_1^{(L)}$影响成本函数：
![[Pasted image 20230201140202.png]]
然后就搞定了，只要算出倒数第二层成本函数对激活值的敏感度，接下来只要重复上述过程计算喂给倒数第二层的权重和偏置就好！


## 2.2 小结

- 成本函数：预测值与目标值的差距
- 权重：每个神经元连接上一个神经元的通道上的数字，越大表示影响越大
- 偏置：一个数字，保证不能随便激发
- 激活值公式：权重、前一个激活值、偏置值放入压缩函数算出
- 权重$w^{(L)}$的微小变化会导致加权和$z^{(L)}$产生变化，然后会导致激活值$a^{(L)}$产生变化，最后影响到cost值。


# 3. 小结

- 反向传播的目的是更新参数，了解权重和偏置如何影响输出值。
- 反向传播的手段是将成本函数对W和b求偏导，也就是求梯度，用梯度下降法来对参数进行更新。
- 根据公式，第l层的参数的梯度，需要通过l+1层的梯度来求得，因此我们求导的过程是“反向”的。



# 参考文献

- [什麼是反向傳播演算法？| 深度學習，第3章](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3)
- [Backpropagation calculus | Chapter 4, Deep learning](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)
- [【DL笔记4】神经网络详解，正向传播和反向传播](https://zhuanlan.zhihu.com/p/41138014)
- [How Does Back-Propagation Work in Neural Networks?](https://towardsdatascience.com/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48)




















