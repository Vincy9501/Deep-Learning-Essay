卷积神经网络（Convolutional Neural Network，CNN）。CNN被用于图像识别、语音识别等各种场合，在图像识别的比赛中，基于深度学习的方法几乎都以CNN为基础。

# 1. 整体结构

CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。
之前介绍的神经网络中，相邻层的所有神经元之间都有连接，这称为**全连接（fully-connected）**。另外，我们用Affine层实现了全连接层。如果使用这个Affine层，一个5层的全连接的神经网络就可以通过图所示的网络结构来实现。

![[Pasted image 20230211181315.png]]

如果是基于CNN的网络，则有如下结构：

![[Pasted image 20230211181338.png]]
CNN 中新增了 Convolution 层 和 Pooling 层。CNN 的 层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling层有时会被省略）。

这可以理解为之前的“Affine - ReLU”连接被替换成了“Convolution - ReLU -（Pooling）”

此外，靠近输出的层中使用了之前的“Affine - ReLU”组合。此外，最后的输出层中使用了之前的“Affine - Softmax”组合。这些都是一般的CNN中比较常见的结构。

# 2. 卷积层

## 2.1 全连接层的问题

全连接层存在什么问题呢？那就是数据的形状被“忽视”了。比如，输入数据是图像时，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。

比如MNIST数据集的例子中，输入图像就是1通道、高28像素、长28像素的（1, 28, 28）形状，但却被排成1列，以784个数据的形式输入到最开始的Affine层。

3维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，所以无法利用与形状相关的信息。

卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此， 在CNN中，可以（有可能）正确理解图像等具有形状的数据。

CNN 中，有时将卷积层的输入输出数据称为特征图（feature map）。卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。

## 2.2 卷积运算

![[Pasted image 20230211182620.png]]

如图所示，卷积运算对输入数据应用滤波器。输入大小是 (4, 4)，滤波器大小是(3, 3)，输出大小是(2, 2)。

而运算顺序如下图所示，对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用（**有时将这个计算称为乘积累加运算**）。

![[Pasted image 20230211182931.png]]

在全连接的神经网络中，除了权重参数，还存在偏置。CNN中，滤波器的参数就对应之前的权重。并且，CNN中也存在偏置。

>[!question]
>滤波器窗口以什么规律滑动？

![[Pasted image 20230211183140.png]]

## 2.3 填充（padding）

在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如0等），这称为填充（padding），是卷积运算中经常会用到的处理。

比如， 下图所示的例子中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅度为1的填充”是指用幅度为1像素的0填充周围。

![[Pasted image 20230211183325.png]]

如果将填充设为2，则输入数据的大小变为(8, 8)；如果将填充设为3，则大小变为(10, 10)。

补充：使用填充主要是为了调整输出的大小。

**为什么要填充？**

因为反复多次卷积运算之后可能输出大小变为1，从而导致无法应用卷积运算。填充保证了空间大小不变的情况下把数据传给下一层

## 2.4 步幅（stride）

应用滤波器的位置间隔称为步幅（stride）。

![[Pasted image 20230211184254.png]]

综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。

假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为 (OH, OW)，填充为P，步幅为S。此时，输出大小可通过式（1）计算：
$$
\begin{equation}
\begin{aligned}
OH &= \frac{H+2P-FH}S+1 \\
OW &= \frac{W+2P-FW}S+1
\end{aligned}
\end{equation}
\tag{1}
$$
虽然只要代入值就可以计算输出大小，但是所设定的值必须使式（1）分别可以除尽。根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。

## 2.5 3维数据的卷积运算

通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。

![[Pasted image 20230211201538.png]]

![[Pasted image 20230211201601.png]]

在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。

## 2.6 结合方块思考

把3维数据表示为多维数组时，书写顺序为（channel, height, width）。

![[Pasted image 20230211201711.png]]

在这个例子中，数据输出是1张特征图。所谓1张特征图，换句话说， 就是通道数为1的特征图。


那么，如果要在通道方向上也拥有多个卷积运算的输出，该怎么做呢？为此，就需要用到多个滤波器（权重）。

![[Pasted image 20230211201802.png]]

**通过应用FN个滤波器，输出特征图也生成了FN个**。如果 将这FN个特征图汇集在一起，就得到了形状为(FN, OH, OW)的方块。将这个方块传给下一层，就是CNN的处理流。

>[!question]
>为什么在通道方向上也拥有多个卷积运算的输出，需要用到多个滤波器呢。
><small>是不是因为如果只考虑二维，那么滤波器在图像上移动。如果需要在通道方向上卷积，那么滤波器就需要在通道方向上移动，也就说需要增加滤波器，因为1维是不能卷积的</small>。

关于卷积运算的滤波器，也必须考虑滤波器的数量。因此，作为4维数据，滤波器的权重数据要按(output_channel, input_ channel, height, width)的顺序书写。比如，通道数为3、大小为5 × 5的滤波器有20个时，可以写成(20, 3, 5, 5)。

卷积运算中（和全连接层一样）存在偏置。

![[Pasted image 20230211201939.png]]

## 2.7 批处理

我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数 据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width) 的顺序保存数据。

![[Pasted image 20230211202015.png]]

# 3. 池化层

池化是缩小高、长方向上的空间的运算。

![[Pasted image 20230211202528.png]]

以上是按步幅2进行2 × 2的Max池化时的处理顺序。“Max 池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。一般来说，池化的窗口大小会 和步幅设定成相同的值。比如，3 × 3的窗口的步幅会设为3，4 × 4的窗口 的步幅会设为4等。

补充：除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，Average池化则是计算目标区域的平均值。 在图像识别领域，主要使用Max池化。

池化层的特征：
- 没有要学习的参数
- 通道数不发生变化
- 对微小的位置变化具有鲁棒性（健壮）
	- 输入数据发生微小偏差时，池化仍会返回相同的结果。

# 4. 卷积层和池化层的实现

## 4.1 4维数组

CNN中各层间传递的数据是4维数据。所谓4维数据，比如数据的形状是(10, 1, 28, 28)，则`x = np.random.rand(10, 1, 28, 28)`，这意味着10个高为28、长为28、通道为1的数据。

>[!question]
>高和长一般是相同的吗？

```python
x = np.random.rand(10, 1, 28, 28) # 随机生成数据
x.shape # (10, 1, 28, 28)
x[0].shape # (1, 28, 28)

# 要访问第1个数据的第1个通道的空间数据，可以写成下面这样
x[0, 0] # 或者x[0][0]
```

CNN中处理的是4维数据，因此卷积运算的实现看上去会很复杂，但是通过使用下面要介绍的im2col这个技巧，问题就会变得很简单。

## 4.2 基于im2col的展开

im2col是一个函数，将输入数据展开以适合滤波器（权重）。

对3维的输入数据应用im2col后，数据转换为2维矩阵（正确地讲，是把包含 批数量的4维数据转换成了2维数据）。


![[Pasted image 20230211203918.png]]

对于输入数据，将应用滤波器的区域（3维方块）横向展开为1列。im2col会在所有应用滤波器的地方进行这个展开处理。

![[Pasted image 20230211203949.png]]

在实际的卷积运算中，滤波器的应用区域几乎都是重叠的。在滤波器的应用区域重叠的情况下，使用im2col展开后，展开后的元素个数会多于原方块的元素个数。

因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。

使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可。

![[Pasted image 20230211204122.png]]

## 4.3 卷积层的实现

im2col这一便捷函数具有以下接口。
im2col (input_data, filter_h, filter_w, stride=1, pad=0)
- input_data
	- 由（数据量，通道，高，长）的4维数组构成的输入数据
- filter_h
	- 滤波器的高
- filter_w
	- 滤波器的长
- stride
	- 步幅
- pad
	- 填充
im2col会考虑滤波器大小、步幅、填充，将输入数据展开为2维数组。

```python
class Convolution:
     def __init__(self, W, b, stride=1, pad=0):
            self.W = W
            self.b = b
            self.stride = stride
            self.pad = pad
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
        
        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T # 滤波器的展开
        out = np.dot(col, col_W) + self.b
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)
    return out
```

卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。 滤波器是 (FN, C, FH, FW)的 4 维形状。另外，FN、C、FH、FW分别是 Filter Number（滤波器数量）、Channel、Filter Height、Filter Width的缩写。

>[!question]
>更新参数还是用反向传播，不仅会对全连接层有影响，对池化层和卷积层也会有影响，由于池化层的各参数不变，其作用只是缩小了卷积层的输出，那也就是说反向传播调整的是滤波器？

展开滤波器的部分如上一张图所示，将各个滤波器的方块纵向展开为1列。这里通过reshape(FN,-1)将参数指定为-1，这是 reshape的一个便利的功能。通过在reshape时指定为-1，reshape函数会自 动计算-1维度上的元素个数，以使多维数组的元素个数前后一致。比如， (10, 3, 5, 5)形状的数组的元素个数共有750个，指定reshape(10,-1)后，就会转换成(10, 75)形状的数组。

转换时使用了 NumPy的transpose函数。transpose会更改多维数组的轴的顺序。通过指定从0开始的索引（编号）序列，就可以更改轴的顺序。

![[Pasted image 20230211212348.png]]

## 4.4 池化层的实现

池化层的实现和卷积层相同，都使用im2col展开输入数据。不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同。

![[Pasted image 20230211212715.png]]

![[Pasted image 20230211212725.png]]

1. 展开输入数据。 
2. 求各行的最大值。
3. 转换为合适的输出大小。

# 5. CNN的实现

![[Pasted image 20230211213156.png]]

网络的构成是“Convolution - ReLU - Pooling -Affine - ReLU - Affine - Softmax”，我们将它实现为名为SimpleConvNet的类。

1. 首先进行初始化，总的来说需要输入数据的维度、卷积层的超参数、全连接层的神经元数量和初始化权重的标准差；各层的权重和偏置；层

初始化时，需要给定如下参数：
- input_dim―输入数据的维度：（通道，高，长）
- conv_param―卷积层的超参数（字典）。字典的关键字如下：
	- filter_num―滤波器的数量
	- filter_size―滤波器的大小
	- stride―步幅
	- pad―填充
- hidden_size―隐藏层（全连接）的神经元数量
- output_size―输出层（全连接）的神经元数量
- weitght_int_std―初始化时权重的标准差

```python
class SimpleConvNet:
    def __init__(self, input_dim=(1, 28, 28),
                 conv_param={'filter_num':30, 'filter_size':5,
                             'pad':0, 'stride':1},
                 hidden_size=100, output_size=10, weight_init_std=0.01):
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / \
 filter_stride + 1
        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))
        
```

```python
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], \
                                                              filter_size, filter_size)
        self.params['b1'] = np.zeros(filter_num)
        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, \
                                                              hidden_size)
        self.params['b2'] = np.zeros(hidden_size)
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, \
                                                              output_size)
        self.params['b3'] = np.zeros(output_size)
```

学习所需的参数是第1层的卷积层和剩余两个全连接层的权重和偏置。将这些参数保存在实例变量的params字典中。
将第1层的卷积层的权重设为关键字W1，偏置设为关键字b1。
分别用关键字W2、b2和关键字W3、b3 来保存第2个和第3个全连接层的权重和偏置。

```python
        self.layers = OrderedDict()
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], \ 
                                           conv_param['stride'], conv_param['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])
        self.last_layer = softmaxwithloss()
```

从最前面开始按顺序向有序字典（OrderedDict）的layers中添加层。只有最后的SoftmaxWithLoss层被添加到别的变量lastLayer中。

所以层的排布是：
input -> Conv1 -> Relu1 -> Pool1 -> Affine1 -> Relu2 -> Affine2 -> softmaxwithloss -> output

2. 初始化之后，进行推理的predict方法和求损失函数的loss方法：
```python
        def predict(self, x):
            for layer in self.layers.values():
                x = layer.forward(x)
            return x
        
        def loss(self, x, t):
            y = self.predict(x)
            return self.lastLayer.forward(y, t)
```

# 7. 具有代表性的CNN

## 7.1 LeNet

它有连续的卷积层和池化层（正确地讲，是只“抽选元素”的子采样层），最后经全连接层输出结果。

和“现在的CNN”相比，LeNet有几个不同点：
- 激活函数
	- sigmoid函数
- 子采样
	- 缩小中间数据的大小

## 7.2 AlexNet

![[Pasted image 20230212162401.png]]

AlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。

- 激活函数使用ReLU。 
- 使用进行局部正规化的LRN（Local Response Normalization）层。 
- 使用Dropout。

# 8. 总结

- CNN在此前的全连接层的网络中新增了卷积层和池化层。 
- 使用im2col函数可以简单、高效地实现卷积层和池化层。 
- 通过CNN的可视化，可知随着层次变深，提取的信息愈加高级。 
- LeNet和AlexNet是CNN的代表性网络。 
- 在深度学习的发展中，大数据和GPU做出了很大的贡献。