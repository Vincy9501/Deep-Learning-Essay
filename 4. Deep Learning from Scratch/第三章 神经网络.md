上一章了解到，感知机可以表示复杂处理，但是设定权重仍需要手工设定。神经网络可以自动从数据中学习到合适的权重参数。

# 1. 从感知机到神经网络

神经网络可以这样表示：

![[Pasted image 20230210165800.png]]

中间层也称为隐藏层。从左到右可以分别表示为0、1、2层。

对于感知机，有：
$$ y=\left\{
\begin{array}{rcl}
0       &      & {(w_1x_1 + w_2x_2 + b)      \le     0}\\
1     &      & {(w_1x_1 + w_2x_2 + b)      \gt     0}\\
\end{array} \right. \tag{1}$$
为了简化，用一个函数h(x)表示该动作：
$$y = h(b+w_1x_1+w_2x_2) \tag{2}$$
$$ h(x)=\left\{
\begin{array}{rcl}
0       &      & {x      \le     0}\\
1     &      & {x      \gt     0}\\
\end{array} \right. \tag{3}$$
h(x)将输入信号总和转换为输出信号，这种函数一般称为激活函数，作用是**决定如何激活输入信号的总和**。

如果将（2）分两阶段进行，先处理输入信号加权总和，再用激活函数转换，则可得到：
$$a = b +w_1x_1+w_2x_2 \tag{4}$$
$$y = h(a) \tag{5}$$
因此有：

![[Pasted image 20230210172151.png]]

补充：一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型。“多层感知机”是指神经网络，即使用 sigmoid 函数等平滑的激活函数的多层网络。

# 2. 激活函数

在（3）中，一旦输出超过阈值就切换输出，如果使用了其他激活函数，那么就可以进入神经网络的世界了。

## 2.1 sigmoid函数

$$h(x) = \frac {1}{1+exp(-x)} \tag{6}$$

这是常用的激活函数，神经网络中用它来进行信号的转换并传给下一个神经元。

## 2.2 阶跃函数和sigmoid函数的实现

![[Pasted image 20230210173155.png]]

![[Pasted image 20230210173355.png]]

sigmoid函数与阶跃函数的区别在于：
- sigmoid是平滑曲线，输出随着输入发生连续性变化
- sigmoid函数可以返回实数值信号，但阶跃函数只能返回0/1
- 但是它们都为非线性函数

**为什么神经网络一定要使用非线性函数做激活函数？**

因为如果使用线性函数的话，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。

## 2.3 ReLU（Rectified Linear Unit）函数

$$ h(x)=\left\{
\begin{array}{rcl}
0       &      & {x      \le     0}\\
x     &      & {x      \gt     0}\\
\end{array} \right. \tag{3}$$
ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0。

# 3. 多维数组运算

numpy提供了一些有用的函数，比如：
- np.array生成数组
- np.dim()获取数组维度
- np.shape获取数组形状
- np.dot()接收两个NumPy数组作为参数，并返回数组的乘积。

我们可以使用NumPy实现神经网络：

![[Pasted image 20230210175258.png]]

```python
X = np.array([1, 2])
W = np.array([[1, 3, 5], [2, 4, 6]])
Y = np.dot(X, W)
#array([ 5, 11, 17])
```

# 4. 三层神经网络实现

![[Pasted image 20230210184251.png]]

1. 符号确认

![[Pasted image 20230210184317.png]]

2. 各层之间信号的传递

![[Pasted image 20230210184346.png]]

$$a_1^{(1)} = w_{11}^{(1)} x_1+ w_{12}^{(1)}x_2 + b_1^{(1)} \tag{8}$$

如果用矩阵：
$$A^{(1)} =XW^{(1)}+B^{(1)} \tag {9}$$
其中：
$$A^{(1)} =(a_1^{(1)} \qquad a_2^{(1)} \qquad a_3^{(1)})$$
$$X = (x_1 \qquad x_2)$$
$$B^{(1)} = (b_1^{(1)} \qquad b_2^{(1)} \qquad b_3^{(1)})$$

$$ W^{(1)} = 
\left(
\begin{matrix} 
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\ 
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)} \\ 
\end{matrix} 
\right)$$
用NumPy实现：


![[Pasted image 20230210185350.png]]

![[Pasted image 20230210185537.png]]

![[Pasted image 20230210185653.png]]

注意，输出层的激活函数用到了恒等函数。一般地，回 归问题可以使用恒等函数，二元分类问题可以使用 sigmoid函数， 多元分类问题可以使用 softmax函数。

整理可得：
```python
def init_network(): 
	network = {} 
	network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
	network['b1'] = np.array([0.1, 0.2, 0.3]) 
	network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) 
	network['b2'] = np.array([0.1, 0.2]) 
	network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
	network['b3'] = np.array([0.1, 0.2])
	return network

def forward(network, x):
	W1, W2, W3 = network['W1'], network['W2'], network['W3']
	b1, b2, b3 = network['b1'], network['b2'], network['b3']
	
	a1 = np.dot(x, W1) + b1
	z1 = sigmoid(a1)
	a2 = np.dot(z1, W2) + b2
	z2 = sigmoid(a2)
	a3 = np.dot(z2, W3) + b3
	y = identity_function(a3)

	return y

network = init_network()
x = np.array([1.0, 0.5]) 
y = forward(network, x) 
print(y) # [ 0.31682708 0.69627909]
```

这里出现了forward（前向）一词，它表示的是从输入到输出方向的传递处理。

# 5. 输出层的设计

## 5.1 恒等函数和softmax函数

恒等函数会将输入按原样输出。

![[Pasted image 20230210190346.png]]

分类问题中使用的softmax函数可以用下面的式（10）表示。
$$y_k = \frac {exp(a_k)} {\sum_{i=1}^n exp(a_i)} \tag{10}$$
表示：假设输出层共有n个神经元，计算第k个神经元的输出$y_k$。

![[Pasted image 20230210190351.png]]

实现softmax函数：

![[Pasted image 20230210190514.png]]

## 5.2 实现softmax函数的注意事项

溢出问题。指数函数的值很容易变得非常大。

因此可以这样改进：
$$
\begin{align}
y_k = \frac {exp(a_k)} {\sum_{i=1}^n exp(a_i)} &= \frac{C exp(a_k)} {C \sum_{i=1}^nexp(a_i)}\\
  &= \frac {exp(a_k+logC)}{\sum_{i=1}^nexp(a_i+logC)} \tag{11}\\
  &= \frac {exp(a_k+C')}{\sum_{i=1}^nexp(a_i+C')} 
\end{align}
$$

首先，式（11）在分子和分母上都乘上C这个任意的常数。
然后，把这个C移动到指数函数（exp）中，记为log C。
最后，把log C替换为另一个符号C'。

在进行softmax的指数函数的运算时，加上（或者减去） 某个常数并不会改变运算的结果。为了防止溢出，一般会使用输入信号中的最大值。

![[Pasted image 20230210191152.png]]

## 5.3 softmax函数的特征

- softmax函数的输出是0.0到1.0之间的实数
- softmax 函数的输出值的总和是1
- 因此可以解释为概率
- 
一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。
并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。
因此， 神经网络在进行分类时，输出层的softmax函数可以省略。

## 5.4 输出层的神经元数量

需要根据待解决的问题来决定。

# 6. 手写数字识别

## 6.1 MNIST数据集

MNIST数据集是由0到9的数字图像构成的。训练图像有6万张， 测试图像有1万张，这些图像可以用于学习和推理。

MNIST数据集的一般使用方法是：
- 先用训练图像进行学习
- 再用学习到的模型度量能在多大程度 对测试图像进行正确的分类

![[Pasted image 20230210193210.png]]

load_mnist函数以“(训练图像 ,训练标签 )，(测试图像，测试标签 )”的形式返回读入的MNIST数据。
load_mnist(normalize=True, flatten=True, one_hot_label=False)
其参数分别为：
1. 是否将输入图像正规化为0.0～1.0的值
2. 是否展开输入图像（变成一维数组）
3. 是否将标签保存为onehot表示（one-hot representation）。one-hot表示是仅正确解标签为1，其余皆为0的数组

![[Pasted image 20230210193630.png]]

flatten=True时读入的图像是以一列（一维）NumPy 数组的形式保存的。因此，显示图像时，需要把它变为原来的28像素 × 28 像素的形状。可以通过reshape()方法的参数指定期望的形状，更改NumPy 数组的形状。此外，还需要把保存为NumPy数组的图像数据转换为PIL用的数据对象，这个转换处理由Image.fromarray()来完成。

## 6.2 推理处理

神经网络的输入层有784个神经元，输出层有10个神经元。这个神经网络有2个隐藏层，第1个隐藏层有50个神经元，第2个隐藏层有100个神经元。
我们先定义get_data()、init_network()、predict()这3个函数。

![[Pasted image 20230210194258.png]]
init_network()会读入保存在pickle文件sample_weight.pkl中的学习到的权重参数。这个文件中以字典变量的形式保存了权重和偏置参数。

现在用这3个函数实现神经网络的推理处理，然后评价它的**识别精度**（accuracy）。

步骤如下：
1. 获得MNIST数据集，生成网络。
2. 用for语句逐一取出保存在x中的图像数据，用predict()函数进行分类。predict()函数以NumPy数组的形式输出各个标签对应的概率。
3. np.argmax(x)取出这个概率列表中的最大值的索引作为预测结果。
4. 比较神经网络所预测的答案和正确解标签，将回答正确的概率作为识别精度。

像这样把数据限定到某个范围内的处理称为**正规化（normalization）**。
对神经网络的输入数据 进行某种既定的转换称为**预处理（pre-processing）**。

![[Pasted image 20230210201924.png]]

## 6.3 批处理

我们来关注输入数据和权重参数的“形状”。

![[Pasted image 20230210203628.png]]

![[Pasted image 20230210203635.png]]

现在我们来考虑打包输入多张图像的情形。比如，我们想用predict() 函数一次性打包处理100张图像。为此，可以把x的形状改为100 × 784，将100张图像打包作为输入数据。

这种打包式的输入数据称为批（batch）。

![[Pasted image 20230210203853.png]]

简单来说就是一批一批处理，通过x[i:i+batch_size]从输入数 据中抽出批数据，然后通过argmax()获取值最大的元素的索引。

# 7. 总结

- 神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数。
- 通过巧妙地使用NumPy多维数组，可以高效地实现神经网络。
- 机器学习的问题大体上可以分为回归问题和分类问题。
- 关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用softmax函数。
- 分类问题中，输出层的神经元的数量设置为要分类的类别数。
- 输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算。

