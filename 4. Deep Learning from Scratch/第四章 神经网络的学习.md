学习：以该损失函数为基准，找出能使它的值达到最小的权重参数。

# 1. 从数据中学习

从数据中学习，就是说可以由数据自动决定权重参数的值。

## 1.1 数据驱动

数据是机器学习的核心，机器学习是由数据驱动的。

我们在面对数字5的时候，如何设计一个能将5正确分类的程序？一种方法是，先从图像中提取**特征量**，再用机器学习技术学习这些特征量的模式。

- 特征量：可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。

神经网络直接学习图像本身。在利用特征量和机器学习的方法中，特征量仍是由人工设计的，而在神经网络中，连 图像中包含的重要特征量也都是由机器来学习的。

![[Pasted image 20230211143037.png]]

##  1.2 训练数据和测试数据

机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。

这是为了能正确评价模型的泛化能力。泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力，是机器学习的最终目标。

只对某个数据集过度拟合的状态称为过拟合（over fitting）。避免过拟合也是机器学习的一个重要课题。

# 2. 损失函数

神经网络以**损失函数**为线索寻找**最优权重参数**。一般用**均方误差**和**交叉熵误差**。

## 2.1 均方误差（mean squared error）

$$E = \frac 1 2 \sum_k (y_k - t_k)^2 \tag{1}$$

$y_k$是神经网络的输出，$t_k$表示监督数据，k表示数据的维度。

补充：将正确解标签表示为1，其他标签表示为0的表示方法称为one-hot表示。

## 2.2 交叉熵误差

$$E = -\sum_k t_k logy_k \tag{2}$$

## 2.3 mini-batch学习

如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式（3）
$$E = -\frac 1 N \sum_n\sum_k t_{nk} logy_{nk} \tag{3}$$
假设数据有N个，$t_{nk}$表示第n个数据的第k个元素的值。只是把求单个数据的损失函数的式（2）扩大到了N份数据。除以N获得平均损失函数。

对于MNIST数据集，有6w个训练数据，全部使用需要耗费大量时间。因此我们从全部数据中选出一部分，作为全部数据的“近似”。这种学习方式叫做**mini-batch学习**。

![[Pasted image 20230211145308.png]]

## 2.4 mini-batch版交叉熵误差的实现

```python
def cross_entropy_error(y, t): 
	if y.ndim == 1: 
		t = t.reshape(1, t.size) 
		y = y.reshape(1, y.size) 
		
	batch_size = y.shape[0] 
	return -np.sum(t * np.log(y + 1e-7)) / batch_size
```
y是神经网络的输出，t是监督数据。当输入为mini-batch时， 要用batch的个数进行正规化，计算单个数据的平均交叉熵误差。

当监督数据是标签形式，交叉熵误差可通过如下代码实现。
```python
def cross_entropy_error(y, t): 
	if y.ndim == 1: 
		t = t.reshape(1, t.size) 
		y = y.reshape(1, y.size) 
	batch_size = y.shape[0] 
	return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```
在t为标签形式时，可用np.log( y[np.arange (batch_size), t] )实现相同的处理。它会生成一个从0到batch_size-1的数组。比如当batch_size为5 时，np.arange(batch_size)会生成一个NumPy 数组[0, 1, 2, 3, 4]。因为t中标签是以[2, 7, 0, 9, 4]的形式存储的，所以y[np.arange(batch_size), t]能抽出各个数据的正确解标签对应的神经网络的输出（在这个例子中， y[np.arange(batch_size), t] 会生成 NumPy 数组 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]）。

## 2.5 设定损失函数的意义

之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数都会变为0，导致参数无法更新。识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。

# 3. 数值微分

## 3.1 导数

导数就是表示某个瞬间的变化量。可以表示为：
$$\frac {df(x)}{dx} = \lim_{h->0} \frac{f(x+h)-f(x)}{h} \tag{4}$$
表示的是函数的导数。
```python
# 不好的实现示例 
def numerical_diff(f, x):
	h = 10e-50
	return (f(x+h) - f(x)) / h
```
numerical_diff来源于**数值微分**，这个函数有两个参数，即“函数f”和“传给函数f的参数x”。

这个函数有两个需要改进的地方：
1. 在上面的实现中，因为想把尽可能小的值赋给h，所以h使用了微小值，但反而产生了**舍入误差**（rounding error）。舍入误差，是指因省略小数的精细部分的数值造成最终计算结果上的误差。
2. 真导数和我们实现的函数之间有差别，因为h不可能无限接近0。

为了减小这个误差，我们可以计算函数f在(x + h)和(x − h)之间的差分。因为这种计算方法以x为中心，计算它左右两边的差分，所以也称为**中心差分**。

![[Pasted image 20230211150311.png]]

## 3.3 偏导数

$$f(x_0,x_0 )= x_0^2+x_1^2 \tag{6}$$
```python
def function_2(x):
	return np.sum(x**2)
```
对于这个函数，有如下图像：

![[Pasted image 20230211150831.png]]

现在求（6）的导数，我们把这里讨论的有多个变量的函数的导数称为偏导数。$\frac {\partial f} {\partial {x_0}}$、$\frac {\partial f} {\partial {x_1}}$。

偏导数和单变量的导数一样，都是求某个地方的斜率。不过， 偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。

## 3.4 梯度

像$(\frac {\partial f} {\partial {x_0}},\frac {\partial f} {\partial {x_1}})$这样的由全部变量的偏导数汇总而成的向量称为**梯度（gradient）**。

实现如下：

![[Pasted image 20230211151951.png]]

**梯度指示的方向是各点处的函数值减小最多的方向**。

### 3.4.1 梯度法

神经网络必须在学习时找到最优参数（权重和偏置）。我们通过梯度来寻找函数最小值 （或者尽可能小的值），这种方法就是梯度法。

梯度，会指向局部最小值。

$$x_0 = x_0 - \eta \frac {\partial f}{\partial {x_0}}$$
$$x_1 = x_1 - \eta \frac {\partial f}{\partial {x_1}} \tag{7}$$

$\eta$表示更新量，在神经网络中称为**学习率（learning rate）**。决定在一次学习中，应该学习多少，以及在多大程度上更新参数。

通过反复执行（7），逐渐减小函数值。

python实现：
![[Pasted image 20230211152525.png]]

设初始值为(-3.0, 4.0)，开始使用梯度法寻找最小值。最终的结 果是(-6.1e-10, 8.1e-10)，非常接近(0，0)。实际上，真的最小值就是(0，0)， 所以说通过梯度法我们基本得到了正确结果。

![[Pasted image 20230211152545.png]]


学习率过大的话，会发散成一个很大的值；反过来，学 习率过小的话，基本上没怎么更新就结束了。也就是说，设定合适的学习率是一个很重要的问题。

补充：像学习率这样的参数称为超参数。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。

### 3.4.2 神经网络的梯度

这里所说的梯度是指损失函数关于权重参数的梯度。
比如，有一个只有一个形状为2 × 3的权重W的神经网络，损失函数用L表示。此时，梯度可以用$\frac{\partial L} {\partial W}$ 表示：
$$
\begin{equation} \begin{aligned}
W &= 
\left(
\begin{matrix}
w_{11} & w_{12} & w_{13} \\ 
w_{21} & w_{22} & w_{23}
\end{matrix}
\right)\\

\frac{\partial L}{\partial W} &=
\left(
\begin{matrix}
\frac{\partial L}{\partial w_{11}} & \frac{\partial L}{\partial w_{12}} & \frac{\partial L}{\partial w_{13}} \\ 
\frac{\partial L}{\partial w_{21}} & \frac{\partial L}{\partial w_{22}} & \frac{\partial L}{\partial w_{23}}
\end{matrix}
\right)

\end{aligned} \end{equation}
\tag{8}
$$

用python实现：

![[Pasted image 20230211165807.png]]

这里我们定义了一个simpleNet类，有一个实例变量W，是形状为2×3的权重参数，它有两个方法，一个是用于预测的predict（x），一个是用于求损失函数值的loss(x,t)。x接收输入数据，t接收正确解标签。

# 5. 学习算法的实现

1. mini-batch
2. 计算梯度
3. 更新参数
4. 重复

因为这里使用的数据是随机选择的mini batch数据，所以又又称为随机梯度下降法（stochastic gradient descent）。

## 5.1 2层神经网络的类

TwolayerNet类中使用的变量：
变量|说明
-|-
params|保存神经网络的参数的字典型变量（实例变量）。<br>params['W1']是第1层的权重，params['b1']是第1层的偏置。<br>params['W2']是第2层的权重，params['b2']是第2层的偏置。
grads|保存梯度的字典型变量（numerical_gradient()方法的返回值）。<br>grads['W1']是第1层权重的梯度，grads['b1']是第1层偏置的梯度。<br>grads['W2']是第2层权重的梯度，grads['b2']是第2层偏置的梯度。

TwoLayerNet类的方法：
方法|说明
-|-
__init__(self, input_size, hidden_size, output_size)|进行初始化。<br>参数从头开始依次表示输入层的神经元数、隐藏层 的神经元数、输出层的神经元数。
predict(self, x)|进行识别（推理）。<br>参数x是图像数据。
loss(self, x, t) |计算损失函数的值。<br>参数x是图像数据，t是正确解标签（后面3个方法的参数也一样）。
accuracy(self, x, t)|计算识别精度。
numerical_gradient(self, x, t)|计算权重参数的梯度。
gradient(self, x, t)|计算权重参数的梯度。

TwoLayerNet类有params和grads两个字典型实例变量。params变量中保存了权重参数。

方法中：
- 初始化方法
- 预测
- 计算损失函数
- 计算识别精度
- 计算梯度

## 5.2 mini-batch的实现

```python
# coding: utf-8
import sys, os
sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

# 读入数据
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000  # 适当设定循环的次数
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 计算梯度
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    
    # 更新参数
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

# 绘制图形
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

![[Pasted image 20230211180714.png]]

随着学习的进行，损失函数的值在不断减小。

## 5.3 基于测试数据的评价

神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合。

因此，每经过一个 epoch，我们都会记录下训练数据和测试数据的识别精度。

![[Pasted image 20230211180948.png]]

实线表示训练数据的识别精度，虚线表示测试数据的识别精度。如图所示，随着epoch的前进（学习的进行），我们发现使用训练数据和 测试数据评价的识别精度都提高了，并且，这两个识别精度基本上没有差异（两 条线基本重叠在一起）。因此，可以说这次的学习中没有发生过拟合的现象。

# 6. 总结

- 机器学习中使用的数据集分为训练数据和测试数据。 
- 神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力。
- 神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小。
- 利用某个给定的微小值的差分求导数的过程，称为数值微分。
- 利用数值微分，可以计算权重参数的梯度。
- 数值微分虽然费时间，但是实现起来很简单。