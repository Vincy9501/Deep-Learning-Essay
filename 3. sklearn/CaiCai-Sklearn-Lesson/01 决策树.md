
# 1. sklearn简介

所需环境：
```python
Python 3.7.1
Scikit-learn 0.20.0
Graphviz 0.8.4 (没有画不出决策树哦，安装代码 conda install python-graphviz）
Numpy 1.15.3
Pandas 0.23.4
Matplotlib 3.0.1
SciPy 1.1.0
```

**scikit-learn**，又写作 **sklearn**，是一个开源的基于python语言的机器学习工具包。它通过 **NumPy**、**SciPy**、**Matplotlib** 等python数值计算的库实现高效的算法应用，并且涵盖了几乎所有主流机器学习算法。

scikit-learn官网：[http://scikit-learn.org/stable/index.html](http://scikit-learn.org/stable/index.html)

在工程应用中：

- 用python手写代码来从头实现一个算法的可能性非常低
- 更多情况下，是分析采集到的数据，根据数据特征选择适合的算法，在工具包中调用算法，调整算法的参数，获取需要的信息，从而实现算法效率和效果之间的平衡

算法相关推荐：

- 数据挖掘导论 - Pang-Ning Tan
- 机器学习 - 周志华

本门课的学习重点：

- sklearn中对算法的说明，调参，属性，接口，以及实例应用
- 本门课不涉及详细的算法原理，只会专注于算法在sklearn中的实现


# 2. 决策树

## 2.1 概述

### 2.1.1 决策树是如何工作的

**决策树 (Decision Tree)** 是一种**非参数**的**有监督学习**方法。
- 非参数：不限制数据的结构和类型，几乎可以用它处理各种各样的数据。

它能够从一系列有**特征**和**标签**的数据中总结出决策规则，并用**树状图的结构**来呈现这些规则，以解决**分类**和**回归**问题。

例：
名字（表索引）、体温等（特征）、类标号（标签）
![[Pasted image 20230203172135.png]]

我们现在的目标是：将动物们分为哺乳类和非哺乳类。根据已经收集到的数据，决策树算法为我们算出了下面的这棵决策树：

![[Pasted image 20230203172314.png]]

我们只需要对其进行提问即可判断它的所属类别。

-  **根节点**：最初的问题所在的地方
-  **中间节点**：在得到结论前的每一个问题
-  **叶子节点**：得到的每一个结论（动物的类别）

> <small>节点：<br>根节点：没有进边，有出边；包含最初的针对特征的提问。<br>中间节点：既有进边也有出边，进边只有一条，出边可以有很多条；都是针对特征的提问。<br>叶子节点：有进边，没有出边；每个叶子节点都是一个类别标签。<br>子节点和父节点：在两个相连的节点中，更接近根节点的是父节点，另一个是子节点。</small>

决策树算法的核心是要解决两个问题：

1.  如何从数据表中找出**最佳节点和最佳分枝**？
	- 提问哪个特征最好？
	- 下一个特征要问哪个？
2.  如何让决策树停止生长，防止**过拟合**？  
	- **过拟合**是指在训练集上表现很好，在测试集上却表现糟糕。是为了得到一致假设而使假设变得过度严格。
	- 这些特征都要问完吗？
	- 什么情况下能总结出最简洁明了的树？

## 2.2 sklearn中的决策树

sklearn中决策树的类都在 `sklearn.tree` 这个模块下。这个模块总共包含五个类：
类|树
-|-
tree.DecisionTreeClassifier|分类树
tree.DecisionTreeRegressor	|回归树
tree.export_graphviz	|将生成的决策树导出为DOT格式，画图专用
tree.ExtraTreeClassifier	|高随机版本的分类树
tree.ExtraTreeRegressor	|高随机版本的回归树

训练步骤：

![[Pasted image 20230203175337.png]]

```python
from sklearn import tree  #导入需要的模块

clf = tree.DecisionTreeClassifier()  #1.实例化,建立评估模型对象:分类树
clf = clf.fit(X_train, y_train)      #2.用训练集数据训练模型
result = clf.score(X_test,y_test)    #3.导入测试集,从接口中调用需要的信息
```

# 3. 分类树 DecisionTreeClassifier

包括决策树的基本流程，分类树的7个参数，1个属性，4个接口，以及绘图代码。

1. 7 个参数：
	- Criterion
	- 2个随机性相关的参数：random_state，splitter
	- 4个剪枝参数：max_depth, min_sample_leaf，max_feature，min_impurity_decrease
2. 1 个属性：**feature_importances_**
3. 4 个接口：**fit**，**score**，**apply**，**predict**

```python
class sklearn.tree.DecisionTreeClassifier (
	criterion='gini', 
	splitter='best', 
	max_depth=None,
	min_samples_split=2, 
	min_samples_leaf=1,
	min_weight_fraction_leaf=0.0, 
	max_features=None,
	random_state=None, 
	max_leaf_nodes=None,
	min_impurity_decrease=0.0, 
	min_impurity_split=None,
	class_weight=None, 
	presort=False
)
```

## 3.1 重要参数

### 3.1.1 criterion

为了要将表格转化为一棵树，决策树需要找出**最佳节点**和**最佳的分枝方法**，对分类树来说，衡量这个“最佳”的指标叫“**不纯度**”。通常来说，不纯度越低，决策树对训练集的拟合越好。

现在的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。

不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且**子节点的不纯度一定是低于父节点的**，也就是说，在同一棵决策树上，**叶子节点的不纯度一定是最低的**。

Criterion 这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：
1. 输入 `entropy`，使用**信息熵（Entropy）**
$$Entropy(t)=\sum_{i=0}^{c-1}​p(i∣t)log_2​p(i∣t)$$
2. 输入`gini`，使用**基尼系数（Gini Impurity）**
$$Gini(t)=1-\sum_{i=0}^{c-1}​​p(i∣t)^2$$
t代表给定的节点，i代表标签的任意分类，p ( i ∣ t )代表标签分类i在节点t上所占的比例。

> <small>信息熵描述了信息源各可能事件发生的不确定性。香农把信息中排除了冗余后的平均信息量称为“信息熵”。<br><br>比如抛硬币的问题，如果用抛硬币来举例。正面反面出现的概率一样，可能的结果为0和1，只需要1bit的信息就能传递数据。<br><br>但假如有八只强队，每个强队赢得比赛的概率都一样，我们就需要三位编码结果，这样就等于八种不同的状态。也就是说如果有M个结果，信息熵就为log2M，在这个例子中，M = 8 = 2^3，信息熵为3。<br><br>如果m均匀分布，有m个可能的结果，每个结果（p1 ... pn）的概率为1/m，因此就熵而言，我们可以将每个结果解释为需要log2 1/p个信息熵来编码。也就是-log2 p，如果用M替换掉p，则为log2M。<br> <br>对于不均匀分配，我们也可以表示。- pi log2 pi 求和，它描述的是平均需要多少信息来描述它所链接的分布的结果。<br> <br>这与不确定性紧密相关，因为如果我们不太确定结果，我们就需要更多的信息来描述它。信息熵的重要性在于它可以提供有用的统计数据，说明我们从观察结果中获得了多少信息，或者告诉我们结果的变化程度。
</small>

> <small>基尼指数（Gini不纯度）表示在样本集合中一个随机选中的样本被分错的概率。Gini指数越小表示集合中被选中的样本被参错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。当集合中所有样本为一个类时，基尼指数为0。<br><br>它的计算过程是：<br>1. 随机从数据集中选出一个点<br>2.  计算数据集中数据分布概率 <br>3. 计算我们把上面随机选取的数据点分错类的概率<br><br> 假如一个数据点正好是第i个分类的概率是p(i)，那么计算基尼不纯度的公式就是上图所示公式。</small>

- 信息熵的计算比基尼系数慢一些，因为基尼系数的计算不涉及对数。
- 当使用信息熵时，sklearn实际计算的是基于信息熵的**信息增益**(Information Gain)，即父节点的信息熵和子节点的信息熵之差。
- 比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是**在实际使用中，信息熵和基尼系数的效果基本相同**。
- 因为信息熵对不纯度更加敏感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此对于高维数据或者噪音很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果往往比较好。当然，这不是绝对的。

参数|criterion
-|-
如何影响模型？|确定不纯度的计算方法，帮忙找出最佳节点和最佳分枝，不纯度越低，决策树对训练集的拟合越好
可能的输入有哪些？|不填默认基尼系数；填写`gini`使用基尼系数，填写`entropy`使用信息增益
怎样选取参数？| 1. 通常就使用基尼系数 <br>2. 数据维度很大，噪音很大时使用基尼系数<br>3. 维度低，数据比较清晰的时候，信息熵和基尼系数没区别<br>4. 当决策树的拟合程度不够的时候，使用信息熵<br>5. 两个都试试，不好就换另外一个

流程总结：

![[Pasted image 20230203180836.png]]

**直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。**

例子见3.1_example.ipyb

我们已经在只了解一个参数的情况下，建立了一棵完整的决策树。但是回到步骤4建立模型，score会在某个值附近波动，引起步骤5中画出来的每一棵树都不一样。它为什么会不稳定呢？如果使用其他数据集，它还会不稳定吗？

我们之前提到过，无论决策树模型如何进化，**在分枝上的本质都还是追求某个不纯度相关的指标的优化**，而正如我们提到的，**不纯度是基于节点来计算的**，也就是说，决策树在建树时，**是靠优化节点来追求一棵优化的树**，但最优的节点能够保证最优的树吗？**集成算法**被用来解决这个问题：sklearn表示，既然一棵树不能保证最优，那就建更多的不同的树，然后从中取最好的。**怎样从一组数据集中建不同的树**？在每次分枝时，不从使用全部特征，而是随机选取一部分特征，从中选取不纯度相关指标最优的作为分枝用的节点。这样，每次生成的树也就不同了。


```python
clf = tree.DecisionTreeClassifier(criterion="entropy",random_state=30)
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) #返回预测的准确度

score
```



### 3.1.2 random_state、splitter

random_state 用来设置**分枝中的随机模式的参数**，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。

splitter 也是用来控制决策树中的随机选项的，有两种输入值，

- 输入best，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看）
- 输入random，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。

当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用**剪枝参数**来防止过拟合。

### 3.1.3 剪枝参数

在不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止。这样的决策树往往会过拟合，这就是说，它会在训练集上表现很好，在测试集上却表现糟糕。我们收集的样本数据不可能和整体的状况完全一致，**因此当一棵决策树对训练数据有了过于优秀的解释性，它找出的规则必然包含了训练样本中的噪声，并使它对未知数据的拟合程度不足**。

```python
# 我们的树对训练集的拟合程度如何？
score_train = clf.score(Xtrain, Ytrain)
score_train
```

为了让决策树有更好的泛化性，我们要对决策树进行剪枝。**剪枝策略对决策树的影响巨大，正确的剪枝策略是优化决策树算法的核心**。sklearn为我们提供了不同的剪枝策略：

1. **max_depth**
	- 限制树的最大深度，超过设定深度的树枝全部剪掉
	- 这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从`=3`开始尝试，看看拟合的效果再决定是否增加设定深度。
2. **min_samples_leaf** & **min_samples_split**
	- **min_samples_leaf**
		- 限定一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生
		- 一般搭配 **max_depth** 使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据
		- 一般来说，建议从`=5`开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用
		- 这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，`=1`通常就是最佳选择
3. **max_features** & **min_impurity_decrease**
	- **max_features** 是限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃
	- 和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法
	- **min_impurity_decrease** 限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的功能，在0.19版本之前时使用**min_impurity_split**

**信息增益：父节点和子节点信息熵的差**

**如何确定最优的剪枝参数？**

-   使用**确定超参数的曲线**来进行判断；  
    继续使用我们已经训练好的决策树模型clf。  
    **超参数的学习曲线**，是一条以**超参数的取值**为**横坐标**，**模型的度量指标**为**纵坐标**的曲线，它是用来衡量不同超参数取值下模型的表现的线。  
    在我们建好的决策树里，我们的模型度量指标就是score。

思考：
1、剪枝参数一定能够提升模型在测试集上的表现吗？
答：调参没有绝对的答案，一切都是看数据本身。

2、这么多参数，一个个画学习曲线？
答：无论如何，剪枝参数的默认值会让树无尽地生长，这些树在某些数据集上对内存的消耗可能非常巨大。所以如果你手中的数据集非常大，你已经预测到无论如何都是要剪枝的，那提前设定这些参数来控制树的复杂性和大小会比较好。

### 3.1.4 目标权重参数 class_weight、min_weight_fraction_leaf

完成样本**标签平衡**的参数。  
**样本不平衡**是指**在一组数据集中，标签的一类天生占有很大的比例**。  
比如说，在银行要判断 “一个办了信用卡的人是否会违约”，就是 “是"vs"否” (1% : 99%) 的比例。这种分类状况下，即便模型什么也不做，全把结果预测成"否"，正确率也能有99%。

- **class_weight**
	-  我们要使用**class_weight**参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。  
	- 该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。
- **min_weight_fraction_leaf**
	- 有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配**min_weight_fraction_leaf**这个基于权重的剪枝参数来使用。
	- 基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。

## 3.2 重要属性和接口

### 3.2.1 重要属性 feature_importances_

属性是在模型训练之后，能够调用查看的模型的各种性质。

对决策树来说，最重要的是`feature_importances_`，能够查看各个特征对模型的重要性。

### 3.2.2 常用接口 fit、score、apply、predict

sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，几乎对每个算法都可以使用。除了这两个接口之外，决策树最常用的接口还有apply和predict。

apply中输入测试集返回每个测试样本所在的叶子节点的索引
predict输入测试集返回每个测试样本的标签，返回的内容一目了然并且非常容易

```python
#apply返回每个测试样本所在的叶子节点的索引
clf.apply(Xtest)

#predict返回每个测试样本的分类/回归结果
clf.predict(Xtest)
```

### 3.2.3 实例：分类树在合成数集上的表现

我们自己生成了三种类型的数据集，第一个是月亮型，两个簇像月亮一样交互在一起；第二个是环型，外面一圈是0；第三个是对半分类型。我们可以跑一下，看决策树天然的对于这些数据分布都有多少效果。

决策树图中有点有线，线是决策树在进行每次的分支的时候画出来的决策边界，颜色越深表示你被分到这一类的可能性越大。

分类树天生不擅长环形数据。最擅长月亮型数据的是最近邻算法，RBF支持向量机和高斯过程；最擅长环形数据的是最近邻算法和高斯过程；最擅长对半分的数据的是朴素贝叶斯，神经网络和随机森林。

# 4. 回归树 DecisionTreeRegressor

## 4.1 重要参数，属性及接口

### 4.1.1 criterion

回归树衡量分枝质量的指标，支持的标准有三种： 
1. 输入"mse"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为 特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失 
2. 输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 
3. 输入"mae"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失 属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心。

$$MSE = \frac{1}{N} \sum_{i = 1}^N(f_i - y_i)^2$$
N - 样本数量，i - 每个数据样本，$f_i$预测值，$y_i$实际数据标签。MSE是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。

在回归中，我们追求的是，MSE越小越好。 然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下：
$$R^2 = 1 - \frac{u}v$$
$$u = \sum_{i=1}^N(f_i - y_i)^2\qquad v=\sum_{i=1}^N(\hat y - y_i)^2$$
yhat 是真实数据标签的平均数。v是总平方和。
R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。

虽然，均方误差永远为正，**但是sklearn当中使用均方误差作为评判标准时，却是计算负均方误差**（neg_mean_squared_error）。

## 4.2 交叉验证

**交叉验证是用来观察模型的稳定性的一种方法**，我们将数据划分为n份，依次使用其中一份作为测试集，其他n-1份作为训练集，多次计算模型的精确性来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此 用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量。

![[Pasted image 20230204185813.png]]

$$E = \frac 1 n \sum_{i=1}^nE_i$$
交叉验证在model_selection这个模块中，导入cross_val_score就可以交叉验证了。

### 4.2.1 交叉验证实例

见4.2.1_example.ipynb

### 4.2.2 泰坦尼克号

见4.2.2_titanic_example.ipynb

# 5. 决策树的优缺点

决策树优点：

1. 易于理解，因为树可以被画出来
2. 需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意，sklearn中的决策树模块不支持对缺失值的处理。
3. 使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是一个很低的成本。
4. 能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类型的数据集。
5. 能够处理多输出问题，即含有多个标签的问题。注意与一个标签中含有多种标签分类的问题区别开。
6. 决策树是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松解释条件。在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。
7. 可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。
8. 即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好。

决策树缺点：

1. 决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据，这称为过度拟合。
	修剪，设置叶节点所需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说会比较晦涩。
2. 决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树。
	这个问题需要通过集成算法来解决。
3. 决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法不能保证返回全局最优决策树。
	这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过程中被随机采样。
4. 有些概念很难学习，因为决策树不容易表达它们，例如XOR，奇偶校验或多路复用器问题。
5. 如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。
	因此，建议在拟合决策树之前平衡数据集。


## 5.1 问题：为什么要划分训练集、测试集和验证集？

按照是否有y标签，可以将模型算法分为有监督算法和无监督算法。其中有监督算法，是根据已有样本标签拟合数据，使得拟合后的误差最小。为了**评估有监督算法的区分度、稳定性、鲁棒性等模型效果**，往往会将样本拆分为：训练集(train set)、测试集(test set)、验证集(validation set 或者 **外样本测试oot**)。

机器学习中普遍的做法是将样本按7:3的比例从同一个样本集中随机划分出训练集和测试集。

这**三个样本集的作用**分别是：

1.  **训练集(train set)**，用于训练有监督模型，拟合模型，调整参数，选择入模变量，以及对算法做出其他抉择；
2.  **测试集(test set)**，用于评估训练出的模型效果，但不会改变模型的参数及效果，一般验证模型是否过拟合或者欠拟合，决定是否重新训练模型或者选择其他的算法；
3.  **验证集(validation set)**，因为训练集和测试集均源自同一分布中，随着时间的流逝，近期样本的分布与训练模型的样本分布会有变化，需要校验训练好的模型在近期样本(验证集)是否有同样的效果，即模型的稳定性、鲁棒性、泛化误差。

所以，**训练模型一定得是在训练集上，而测试集是验证模型在与训练集同分布的样本集上效果，验证集是验证模型在与训练集变化了的样本分布上的效果，评估模型效果是否稳定**。




# 6. 总结

- 决策树：**非参数**的**有监督学习**方法，从一系列有**特征**和**标签**的数据中总结出决策规则，并用**树状图的结构**来呈现这些规则，以解决**分类**和**回归**问题。
- 简而言之，决策树就是**以图的形式呈现从数据集中总结出的决策规则**。
- 决策树的实现中，要思考：怎样获得最好的分枝方法、长到什么时候为止？怎样防止过拟合，即在训练集上表现好在测试集上表现差？
- 对于分类树：
	1. 衡量最佳的指标是不纯度，越低越好。基本上决策树就是找不纯度，对不纯度有关的参数进行调参。
		- 用criterion对不纯度进行计算，有两种方法：信息熵和基尼系数。
		- 通常用基尼，噪声大用基尼，拟合程度低用信息熵
	2. 随机选项有两个，一个是random_state一个是splitter，前者设置随机模式，后者设置随机选项（best用更重要特征分支、random随机）。目的就是防止过拟合。
	3. 为了泛化性，进行剪枝。剪枝参数有：
		- **max_depth**限制最大深度
		- **min_samples_leaf** & **min_samples_split**
		- **max_features** & **min_impurity_decrease**（前者通常不用，后者是最小信息增益，就是父子节点信息熵差）
		- 确定最优的剪枝参数可以用**超参数的曲线**来判断。
	4. 目标权重参数为了解决标签某类天生占很大比例的问题。
		- **class_weight**给一定均衡
		- **min_weight_fraction_leaf**基于权重的剪枝参数更少偏于主导类
- 分类树重要属性和接口：
	- feature_importances_模型训练后查看性质
	- fit、score、apply、predict（后两者返回测试集索引和标签）
- 对于回归树：
	- MSE最小（均方误差）
	- 用交叉验证观察模型稳定性，交叉验证时把数据集分为n份，依次使用其中一份作为测试集，其他n-1份作为训练集，多次计算精确性。

实际上建模就三步：
- 实例化
- 训练模型
- 返回预测准确度

代码总结：
```python
"""
以红酒数据集为例

基本类的导入：
1. 生成数据集的类：from sklearn.datasets import load_wine
2. 模型选择：from sklearn.model_selection import train_test_split
3. 数据结构及可视化：import pandas as pd
4. 绘图：import graphviz

模型相关操作：
1. 模型特征：wine.data.shape
2. 输出：wine.target
3. 输入和输出左右拼接：pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1)
4. 特征名：wine.feature_names
5. 标签名：wine.target_names
6. 划分集合：Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3)

建模；
1. clf = tree.DecisionTreeClassifier(criterion="entropy") # 建模
2. clf = clf.fit(Xtrain, Ytrain) # 训练模型
3. score = clf.score(Xtest, Ytest) # 返回预测的准确度

绘画：
dot_data = tree.export_graphviz(clf
                               ,feature_names = feature_name
                               ,class_names = ['琴酒', '雪莉', '贝尔摩德']
                                ,filled = True
                                ,rounded = True
                               )
graph = graphviz.Source(dot_data)

graph

特征重要性及探索：
1. clf.feature_importances_ # 特征重要性
2. [*zip(feature_name, clf.feature_importances_)] # 返回元组
"""
```

实例：一维回归的图像
```python
"""
1. 导库，通常是numpy sklearn matplotlib
2. 创建曲线，先创建一组随机的，分布在0~5上的横坐标轴的取值(x)，然后将这一组值放到sin函数中去生成纵坐标的值(y)，接着再到y上去添加噪声。
3. 实例化，训练模型
4. 测试集导入模型，预测结果
5. 绘图
"""
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt


rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80,1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))

regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_1.fit(X, y)
regr_2.fit(X, y)

# 有序数列 arange(start,end,rate) np.newaxis增维
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)

```

实例：泰坦尼克号预测
```python
"""
1. 导库
2. 数据集
3. 预处理
4. 提取标签和特征矩阵，分测试集和训练集
5. 导入模型，粗略跑一下查看结果
6. 在不同max_depth下观察模型的拟合状况
7. 网格搜索调整参数
"""
# 除了老三样之外还有分集、网格搜索、交叉验证
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

data = pd.read_csv(r"E:\9-Python\study\notes\Deep-Learning-Essay\3. sklearn\CaiCai-Sklearn-Lesson\train.csv",index_col
= 0)

data.drop(["Cabin","Name","Ticket"],inplace=True,axis=1)
data["Age"] = data["Age"].fillna(data["Age"].mean())
data = data.dropna()
# 二分类直接用boolean转int
data["Sex"] = (data["Sex"]== "male").astype("int")
# 三分类压缩
labels = data["Embarked"].unique().tolist()
data["Embarked"] = data["Embarked"].apply(lambda x: labels.index(x))

# 标签取出来，加个索引
X = data.iloc[:,data.columns != "Survived"]
y = data.iloc[:,data.columns == "Survived"]

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3)
for i in [Xtrain, Xtest, Ytrain, Ytest]:
    i.index = range(i.shape[0])
    
# 跑一下
clf = DecisionTreeClassifier(random_state=25)
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest)
score

clf = DecisionTreeClassifier(random_state=25)
score = cross_val_score(clf,X,y,cv=10).mean()
score

tr = []
te = []
for i in range(10):
    clf = DecisionTreeClassifier(random_state=25
                                 ,max_depth=i+1
                                 ,criterion="entropy"
                               )
    clf = clf.fit(Xtrain, Ytrain)
    score_tr = clf.score(Xtrain,Ytrain)
    score_te = cross_val_score(clf,X,y,cv=10).mean()
    tr.append(score_tr)
    te.append(score_te)
print(max(te))
plt.plot(range(1,11),tr,color="red",label="train")
plt.plot(range(1,11),te,color="blue",label="test")
plt.legend()
plt.show()

gini_thresholds = np.linspace(0,0.5,20)
parameters = {'splitter':('best','random')
             ,'criterion':("gini","entropy")
             ,"max_depth":[*range(1,10)]
             ,'min_samples_leaf':[*range(1,50,5)]
             ,'min_impurity_decrease':[*np.linspace(0,0.5,20)]
             }
clf = DecisionTreeClassifier(random_state=25)
GS = GridSearchCV(clf, parameters, cv=10)

GS.fit(Xtrain,Ytrain)
GS.best_params_
GS.best_score_
```


# 参考文献

- [机器学习为什么要划分训练集、测试集和验证集？这3个样本集的区别又在哪里？](https://zhuanlan.zhihu.com/p/377396096)
- [Intuitively Understanding the Shannon Entropy](https://www.youtube.com/watch?v=0GCGaw0QOhA)
- [The Gini Impurity Index explained in 8 minutes!](https://www.youtube.com/watch?v=u4IxOk2ijSs)