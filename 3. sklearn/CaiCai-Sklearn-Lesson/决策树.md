
# 1. sklearn简介

所需环境：
```python
Python 3.7.1
Scikit-learn 0.20.0
Graphviz 0.8.4 (没有画不出决策树哦，安装代码 conda install python-graphviz）
Numpy 1.15.3
Pandas 0.23.4
Matplotlib 3.0.1
SciPy 1.1.0
```

**scikit-learn**，又写作 **sklearn**，是一个开源的基于python语言的机器学习工具包。它通过 **NumPy**、**SciPy**、**Matplotlib** 等python数值计算的库实现高效的算法应用，并且涵盖了几乎所有主流机器学习算法。

scikit-learn官网：[http://scikit-learn.org/stable/index.html](http://scikit-learn.org/stable/index.html)

在工程应用中：

- 用python手写代码来从头实现一个算法的可能性非常低
- 更多情况下，是分析采集到的数据，根据数据特征选择适合的算法，在工具包中调用算法，调整算法的参数，获取需要的信息，从而实现算法效率和效果之间的平衡

算法相关推荐：

- 数据挖掘导论 - Pang-Ning Tan
- 机器学习 - 周志华

本门课的学习重点：

- sklearn中对算法的说明，调参，属性，接口，以及实例应用
- 本门课不涉及详细的算法原理，只会专注于算法在sklearn中的实现


# 2. 决策树

## 2.1 概述

### 2.1.1 决策树是如何工作的

**决策树 (Decision Tree)** 是一种**非参数**的**有监督学习**方法。
- 非参数：不限制数据的结构和类型，几乎可以用它处理各种各样的数据。

它能够从一系列有**特征**和**标签**的数据中总结出决策规则，并用**树状图的结构**来呈现这些规则，以解决**分类**和**回归**问题。

例：
名字（表索引）、体温等（特征）、类标号（标签）
![[Pasted image 20230203172135.png]]

我们现在的目标是：将动物们分为哺乳类和非哺乳类。根据已经收集到的数据，决策树算法为我们算出了下面的这棵决策树：

![[Pasted image 20230203172314.png]]

我们只需要对其进行提问即可判断它的所属类别。

-  **根节点**：最初的问题所在的地方
-  **中间节点**：在得到结论前的每一个问题
-  **叶子节点**：得到的每一个结论（动物的类别）

> <small>节点：<br>根节点：没有进边，有出边；包含最初的针对特征的提问。<br>中间节点：既有进边也有出边，进边只有一条，出边可以有很多条；都是针对特征的提问。<br>叶子节点：有进边，没有出边；每个叶子节点都是一个类别标签。<br>子节点和父节点：在两个相连的节点中，更接近根节点的是父节点，另一个是子节点。</small>

决策树算法的核心是要解决两个问题：

1.  如何从数据表中找出最佳节点和最佳分枝？
	- 提问哪个特征最好？
	- 下一个特征要问哪个？
2.  如何让决策树停止生长，防止**过拟合**？  
	- **过拟合**是指在训练集上表现很好，在测试集上却表现糟糕。是为了得到一致假设而使假设变得过度严格。
	- 这些特征都要问完吗？
	- 什么情况下能总结出最简洁明了的树？

## 2.2 sklearn中的决策树

sklearn中决策树的类都在 `sklearn.tree` 这个模块下。这个模块总共包含五个类：
类|树
-|-
tree.DecisionTreeClassifier|分类树
tree.DecisionTreeRegressor	|回归树
tree.export_graphviz	|将生成的决策树导出为DOT格式，画图专用
tree.ExtraTreeClassifier	|高随机版本的分类树
tree.ExtraTreeRegressor	|高随机版本的回归树

训练步骤：

![[Pasted image 20230203175337.png]]

```python
from sklearn import tree  #导入需要的模块

clf = tree.DecisionTreeClassifier()  #1.实例化,建立评估模型对象:分类树
clf = clf.fit(X_train, y_train)      #2.用训练集数据训练模型
result = clf.score(X_test,y_test)    #3.导入测试集,从接口中调用需要的信息
```

# 3. 分类树 DecisionTreeClassifier

包括决策树的基本流程，分类树的7个参数，1个属性，4个接口，以及绘图代码。

1. 7 个参数：
	- Criterion
	- 2个随机性相关的参数：random_state，splitter
	- 4个剪枝参数：max_depth, min_sample_leaf，max_feature，min_impurity_decrease
2. 1 个属性：**feature_importances_**
3. 4 个接口：**fit**，**score**，**apply**，**predict**

```python
class sklearn.tree.DecisionTreeClassifier (
	criterion='gini', 
	splitter='best', 
	max_depth=None,
	min_samples_split=2, 
	min_samples_leaf=1,
	min_weight_fraction_leaf=0.0, 
	max_features=None,
	random_state=None, 
	max_leaf_nodes=None,
	min_impurity_decrease=0.0, 
	min_impurity_split=None,
	class_weight=None, 
	presort=False
)
```

## 3.1 重要参数

### 3.1.1 criterion

为了要将表格转化为一棵树，决策树需要找出**最佳节点**和**最佳的分枝方法**，对分类树来说，衡量这个“最佳”的指标叫“**不纯度**”。通常来说，不纯度越低，决策树对训练集的拟合越好。

现在的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。

不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且**子节点的不纯度一定是低于父节点的**，也就是说，在同一棵决策树上，**叶子节点的不纯度一定是最低的**。

Criterion 这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：
1. 输入 `entropy`，使用**信息熵（Entropy）**
$$Entropy(t)=\sum_{i=0}^{c-1}​p(i∣t)log_2​p(i∣t)$$
2. 输入`gini`，使用**基尼系数（Gini Impurity）**
$$Gini(t)=1-\sum_{i=0}^{c-1}​​p(i∣t)^2$$
t代表给定的节点，i代表标签的任意分类，p ( i ∣ t )代表标签分类i在节点t上所占的比例。

- 信息熵的计算比基尼系数慢一些，因为基尼系数的计算不涉及对数。
- 当使用信息熵时，sklearn实际计算的是基于信息熵的**信息增益**(Information Gain)，即父节点的信息熵和子节点的信息熵之差。
- 比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是**在实际使用中，信息熵和基尼系数的效果基本相同**。
- 因为信息熵对不纯度更加敏感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此对于高维数据或者噪音很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果往往比较好。当然，这不是绝对的。

参数|criterion
-|-
如何影响模型？|确定不纯度的计算方法，帮忙找出最佳节点和最佳分枝，不纯度越低，决策树对训练集的拟合越好
可能的输入有哪些？|不填默认基尼系数；填写`gini`使用基尼系数，填写`entropy`使用信息增益
怎样选取参数？| 1. 通常就使用基尼系数 <br>2. 数据维度很大，噪音很大时使用基尼系数<br>3.维度低，数据比较清晰的时候，信息熵和基尼系数没区别<br>4.当决策树的拟合程度不够的时候，使用信息熵<br>5.两个都试试，不好就换另外一个

流程总结：

![[Pasted image 20230203180836.png]]

**直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。**

例子见3.1_example.ipyb

我们已经在只了解一个参数的情况下，建立了一棵完整的决策树。但是回到步骤4建立模型，score会在某个值附近波动，引起步骤5中画出来的每一棵树都不一样。它为什么会不稳定呢？如果使用其他数据集，它还会不稳定吗？

我们之前提到过，无论决策树模型如何进化，**在分枝上的本质都还是追求某个不纯度相关的指标的优化**，而正如我们提到的，**不纯度是基于节点来计算的**，也就是说，决策树在建树时，**是靠优化节点来追求一棵优化的树**，但最优的节点能够保证最优的树吗？**集成算法**被用来解决这个问题：sklearn表示，既然一棵树不能保证最优，那就建更多的不同的树，然后从中取最好的。**怎样从一组数据集中建不同的树**？在每次分枝时，不从使用全部特征，而是随机选取一部分特征，从中选取不纯度相关指标最优的作为分枝用的节点。这样，每次生成的树也就不同了。


```python
clf = tree.DecisionTreeClassifier(criterion="entropy",random_state=30)
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) #返回预测的准确度

score
```



### 3.1.2 random_state、splitter

random_state 用来设置**分枝中的随机模式的参数**，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。

splitter 也是用来控制决策树中的随机选项的，有两种输入值，

- 输入best，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看）
- 输入random，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。

当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用**剪枝参数**来防止过拟合。

### 3.1.3 剪枝参数

在不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止。这样的决策树往往会过拟合，这就是说，它会在训练集上表现很好，在测试集上却表现糟糕。我们收集的样本数据不可能和整体的状况完全一致，**因此当一棵决策树对训练数据有了过于优秀的解释性，它找出的规则必然包含了训练样本中的噪声，并使它对未知数据的拟合程度不足**。

```python
# 我们的树对训练集的拟合程度如何？
score_train = clf.score(Xtrain, Ytrain)
score_train
```

为了让决策树有更好的泛化性，我们要对决策树进行剪枝。**剪枝策略对决策树的影响巨大，正确的剪枝策略是优化决策树算法的核心**。sklearn为我们提供了不同的剪枝策略：

1. **max_depth**
	- 限制树的最大深度，超过设定深度的树枝全部剪掉
	- 这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从`=3`开始尝试，看看拟合的效果再决定是否增加设定深度。
2. **min_samples_leaf** & **min_samples_split**
	- **min_samples_leaf**
		- 限定一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生
		- 一般搭配 **max_depth** 使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据
		- 一般来说，建议从`=5`开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用
		- 这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，`=1`通常就是最佳选择
3. **max_features** & **min_impurity_decrease**
	- **max_features** 是限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃
	- 和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法
	- **min_impurity_decrease** 限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的功能，在0.19版本之前时使用**min_impurity_split**

**信息增益：父节点和子节点信息熵的差**

**如何确定最优的剪枝参数？**

-   使用**确定超参数的曲线**来进行判断；  
    继续使用我们已经训练好的决策树模型clf。  
    **超参数的学习曲线**，是一条以**超参数的取值**为**横坐标**，**模型的度量指标**为**纵坐标**的曲线，它是用来衡量不同超参数取值下模型的表现的线。  
    在我们建好的决策树里，我们的模型度量指标就是score。

思考：
1、剪枝参数一定能够提升模型在测试集上的表现吗？
答：调参没有绝对的答案，一切都是看数据本身。

2、这么多参数，一个个画学习曲线？
答：无论如何，剪枝参数的默认值会让树无尽地生长，这些树在某些数据集上对内存的消耗可能非常巨大。所以如果你手中的数据集非常大，你已经预测到无论如何都是要剪枝的，那提前设定这些参数来控制树的复杂性和大小会比较好。

### 3.1.4 目标权重参数 class_weight、min_weight_fraction_leaf

完成样本**标签平衡**的参数。  
**样本不平衡**是指**在一组数据集中，标签的一类天生占有很大的比例**。  
比如说，在银行要判断 “一个办了信用卡的人是否会违约”，就是 “是"vs"否” (1% : 99%) 的比例。这种分类状况下，即便模型什么也不做，全把结果预测成"否"，正确率也能有99%。

- **class_weight**
	-  我们要使用**class_weight**参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。  
	- 该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。
- **min_weight_fraction_leaf**
	- 有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配**min_weight_fraction_leaf**这个基于权重的剪枝参数来使用。
	- 基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。
- 

## 3.2 重要属性和接口

### 3.2.1 重要属性 feature_importances_

属性是在模型训练之后，能够调用查看的模型的各种性质。

对决策树来说，最重要的是`feature_importances_`，能够查看各个特征对模型的重要性。

### 3.2.2 常用接口 fit、score、apply、predict

sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，几乎对每个算法都可以使用。除了这两个接口之外，决策树最常用的接口还有apply和predict。

apply中输入测试集返回每个测试样本所在的叶子节点的索引
predict输入测试集返回每个测试样本的标签，返回的内容一目了然并且非常容易


## 3.2 问题：为什么要划分训练集、测试集和验证集？

按照是否有y标签，可以将模型算法分为有监督算法和无监督算法。其中有监督算法，是根据已有样本标签拟合数据，使得拟合后的误差最小。为了**评估有监督算法的区分度、稳定性、鲁棒性等模型效果**，往往会将样本拆分为：训练集(train set)、测试集(test set)、验证集(validation set 或者 **外样本测试oot**)。

机器学习中普遍的做法是将样本按7:3的比例从同一个样本集中随机划分出训练集和测试集。

这**三个样本集的作用**分别是：

1.  **训练集(train set)**，用于训练有监督模型，拟合模型，调整参数，选择入模变量，以及对算法做出其他抉择；
2.  **测试集(test set)**，用于评估训练出的模型效果，但不会改变模型的参数及效果，一般验证模型是否过拟合或者欠拟合，决定是否重新训练模型或者选择其他的算法；
3.  **验证集(validation set)**，因为训练集和测试集均源自同一分布中，随着时间的流逝，近期样本的分布与训练模型的样本分布会有变化，需要校验训练好的模型在近期样本(验证集)是否有同样的效果，即模型的稳定性、鲁棒性、泛化误差。

所以，**训练模型一定得是在训练集上，而测试集是验证模型在与训练集同分布的样本集上效果，验证集是验证模型在与训练集变化了的样本分布上的效果，评估模型效果是否稳定**。


## 3.3 问题：什么是过拟合？

过拟合

# 4. 实例

见



## 4.2 

# 5. 决策树的优缺点

# 6. 附录

# 参考文献

- [机器学习为什么要划分训练集、测试集和验证集？这3个样本集的区别又在哪里？](https://zhuanlan.zhihu.com/p/377396096)
- 