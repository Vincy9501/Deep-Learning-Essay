# 1. 概述

## 1.1 数据挖掘五大流程

1. 获取数据
2. 数据预处理
3. 特征工程
4. 建模
5. 上限

## 1.2 sklearn中的数据预处理和特征工程

- 模块preprocessing：几乎包含数据预处理的所有内容
- 模块Impute：填补缺失值专用 
- 模块feature_selection：包含特征选择的各种方法的实践
- 模块decomposition：包含降维算法

# 2. 数据预处理

## 2.1 数据无量纲化

无量纲化：将不同规格的数据转换到同一规格/不同分布的数据转换到某个特定分布

- 分类
	- 线性：中心化处理/缩放处理
		- 中心化：让所有记录减去一个固定值，即让数据样本平移到某个位置
		- 缩放：通过除以一个固定值，将数据固定到某个范围中
	- 非线性

### 2.1.1 preprocessing.MinMaxScaler

**归一化**（中心化+缩放）：数据按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间

$$x^{*} = \frac {x-min(x)}{max(x)-min(x)}$$
代码：
```python
from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]

import pandas as pd 
pd.DataFrame(data)

#实现归一化 
scaler = MinMaxScaler() #实例化 
scaler = scaler.fit(data) #fit，在这里本质是生成min(x)和max(x) 
result = scaler.transform(data) #通过接口导出结果
result

result_ = scaler.fit_transform(data) #训练和导出结果一步达成
scaler.inverse_transform(result) #将归一化后的结果逆转（还原矩阵）

data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] 
scaler = MinMaxScaler(feature_range=[5,10]) #依然实例化
result = scaler.fit_transform(data) #fit_transform一步导出结果 
result

#当X中的特征数量非常多的时候，fit会报错并表示，数据量太大了我计算不了 
#此时使用partial_fit作为训练接口 
#scaler = scaler.partial_fit(data)
```

BONUS: 使用numpy来实现归一化

```python
import numpy as np X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]]) #归一化 
X_nor = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) X_nor 
#逆转归一化 
X_returned = X_nor * (X.max(axis=0) - X.min(axis=0)) + X.min(axis=0) X_returned
```

### 2.1.2 preprocessing.StandardScaler

当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布），而这个过程，就叫做当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分 布），而这个过程，就叫做数据标准化(Standardization，又称Z-score normalization)，公式如下：(Standardization，又称Z-score normalization)，公式如下：
$$x^{*} = \frac {x-\mu}{\sigma}$$
```python
from sklearn.preprocessing import StandardScaler 
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]

scaler = StandardScaler() #实例化 
scaler.fit(data) #fit，本质是生成均值和方差

scaler.mean_ #查看均值的属性mean_ 
scaler.var_ #查看方差的属性var_

x_std = scaler.transform(data) #通过接口导出结果

x_std.mean() #导出的结果是一个数组，用mean()查看均值 
x_std.std() #用std()查看方差

scaler.fit_transform(data) #使用fit_transform(data)一步达成结果

scaler.inverse_transform(x_std) #使用inverse_transform逆转标准化
```

对于StandardScaler和MinMaxScaler来说，空值NaN会被当做是缺失值，在fit的时候忽略，在transform的时候保持缺失NaN的状态显示。

### 2.1.3 StandardScaler和MinMaxScaler怎么选？

看情况。大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。

MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像 处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。

建议先试试看StandardScaler，效果不好换MinMaxScaler。

## 2.2 缺失值

### 2.2.1 impute.SimpleImputer

*class sklearn.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True)

这个类是专门用来填补缺失值的。它包括四个重要参数：

参数|含义&输入
-|-
missing_values |告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan 
strategy |我们填补缺失值的策略，默认均值。 <br>输入“mean”使用均值填补（仅对数值型特征可用） <br>输入“median"用中值填补（仅对数值型特征可用） <br>输入"most_frequent”用众数填补（对数值型和字符型特征都可用） <br>输入“constant"表示请参考参数“fill_value"中的值（对数值型和字符型特征都可用） 
fill_value|当参数startegy为”constant"的时候可用，可输入字符串或数字表示要填充的值，常用0 
copy |默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去。

## 2.3 处理分类型特征：编码与哑变量

大多数的算法，逻辑回归、支持向量机只能够处理数值型数据，不能处理文字，在sklearn中，除了用来处理文字的算法，其他算法在fit的时候要求全部输入数组或矩阵，不能导入文字型数据。

决策树和朴素贝叶斯可以处理文字，但是sklearn中规定必须导入数值型。我们必须将数据进行编码，即是说，将文字型数据转换为数值型。

### 2.3.1 preprocessing.LabelEncoder

将分类转换为分类数值

```python
from sklearn.preprocessing import LabelEncoder

y = data.iloc[:,-1] #要输入的是标签，不是特征矩阵，所以允许一维

le = LabelEncoder() #实例化 
le = le.fit(y) #导入数据 
label = le.transform(y)   #transform接口调取结果

le.classes_ #属性.classes_查看标签中究竟有多少类别 
label #查看获取的结果label

le.fit_transform(y) #也可以直接fit_transform一步
le.inverse_transform(label) #使用inverse_transform可以逆转

data.iloc[:,-1] = label #让标签等于我们运行出来的结果
data.head()

#如果不需要教学展示的话我会这么写： 
from sklearn.preprocessing import LabelEncoder 
data.iloc[:,-1] = LabelEncoder().fit_transform(data.iloc[:,-1])

```

### 2.3.2 preprocessing.OrdinalEncoder

将分类特征转换为分类数值

```python
from sklearn.preprocessing import OrdinalEncoder

#接口categories_对应LabelEncoder的接口classes_，一模一样的功能 
data_ = data.copy()

data_.head()

OrdinalEncoder().fit(data_.iloc[:,1:-1]).categories_

data_.iloc[:,1:-1] = OrdinalEncoder().fit_transform(data_.iloc[:,1:-1])

data_.head()
```

### 问题：Label Encoding 和 Ordinal Encoding的区别

- 两者具有相同的功能。有点不同的是背后的想法。`OrdinalEncoder`用于转换特征，而`LabelEncoder`用于转换目标变量。
- `OrdinalEncoder`用于具有形状的二维数据`(n_samples, n_features)`
- `LabelEncoder`适用于具有形状的一维数据`(n_samples,)`

### 2.3.3 preprocessing.OneHotEncoder

独热编码，创建哑变量（简单来说就是把多分类变量转换为二分变量的一种形式）

>[!question]
>什么是哑变量？
><small>虚拟变量 ( Dummy Variables) 又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。</small>

我们来思考三种不同性质的分类数据：
1.  舱门（S，C，Q） 
三种取值S，C，Q是相互独立的，彼此之间完全没有联系，表达的是S≠C≠Q的概念。这是名义变量。
2. 学历（小学，初中，高中）
在性质上可以有高中>初中>小学这样的联系，但是学历取值之间却不是可以计算的，这是有序变量。
3. 体重（>45kg，>90kg，>135kg）
各个取值之间有联系，且是可以互相计算的，比如120kg - 45kg = 90kg，分类之间可以通过数学计算互相转换。是有距变量。

类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息：

![[Pasted image 20230208165404.png]]

```python
enc.get_feature_names() #返回每一个稀疏矩阵列的名字
```

## 2.4 处理连续型特征：二值化与分段

根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。

默认阈值为0时，特征中所有的正值都映射到1。

二值化是对文本计数数据的常见操作。

### 2.4.1 preprocessing.Binarizer

```python
data_2 = data.copy()

from sklearn.preprocessing import Binarizer X = data_2.iloc[:,0].values.reshape(-1,1) #类为特征专用，所以不能使用一维数组 
transformer = Binarizer(threshold=30).fit_transform(X)

transformer
```

### 2.4.2 preprocessing.KBinsDiscretizer

这是将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。

参数|含义&输入 
-|-
n_bins|每个特征中分箱的个数，默认5，一次会被运用到所有导入的特征
encode|编码的方式，默认“onehot” <br>"onehot"：做哑变量，之后返回一个稀疏矩阵，每一列是一个特征中的一个类别，含有该类别的样本表示为1，不含的表示为0 <br>“ordinal”：每个特征的每个箱都被编码为一个整数，返回每一列是一个特征，每个特征下含 有不同整数编码的箱的矩阵 <br>"onehot-dense"：做哑变量，之后返回一个密集数组。
strategy|用来定义箱宽的方式，默认"quantile"<br>"uniform"：表示等宽分箱，即每个特征中的每个箱的最大值之间的差为 (特征.max() - 特征.min())/(n_bins)<br>"quantile"：表示等位分箱，即每个特征中的每个箱内的样本数量都相同<br>"kmeans"：表示按聚类分箱，每个箱中的值到最近的一维k均值聚类的簇心得距离都相同

# 3. 特征选择 feature_selection

特征提取 (feature extraction) | 特征创造 (feature creation)|特征选择 (feature selection)
-|-|-
从文字，图像，声音等其他非结构化数据中提取新信息作为特征。比如说，从淘宝宝贝的名称中提取出 产品类别，产品颜色，是否是网红产品等等。|把现有特征进行组合，或互相计算，得到新的特征。比如说，我们有一列特征是速度，一列特征是距离，我们就可以通过让两列相处，创造新的特征：通过距离所花的时间。|从所有的特征中，选择出有意义，对模型有帮助的特征，以避免必须将所有特征都导入模型 去训练的情况。

特征工程第一步：理解业务

## 3.1 Filter过滤法

过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法。

它是根据各种统计检验中的分数以及相关性的各项指标来选择特征。

流程：

**全部特征 --> 最佳特征子集 --> 算法 --> 模型评估**

### 3.1.1 方差过滤

#### 3.1.1.1 VarianceThreshold

通过特征本身的方差筛选特征的类，如果方差很小那就说明样本在这个特征上没什么差异，就对样本区分没什么作用。

threshold表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0。

```python
from sklearn.feature_selection import VarianceThreshold 
selector = VarianceThreshold() #实例化，不填参数默认方差为0 
X_var0 = selector.fit_transform(X) #获取删除不合格特征之后的新特征矩阵
```

如果想留下一半特征：
```python
import numpy as np X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
```

当特征是二分类的时候，特征的取值就是伯努利随机变量，这些变量的方差可以计算为：
$$Var[X] = p(1-p)$$
#### 3.1.1.2 方差过滤对模型的影响

KNN和随机森林在方差过滤上有不同的表现，在多特征的情况下，KNN运行时间远远大于随机森林。这是因为随机森林只会选取固定数量的特征建模。

因此：
过滤法的对象：需要遍历特征或升维的算法
过滤法的目标：在维持算法表现的前提下，帮助算法们降低计算成本

KNN：是K近邻算法中的分类算法，其原理非常简单，是利用每个样本到其他样本点的距离来判断每个样本点的相似度，然后对样本进行分类。

>[!question]
>过滤法对随机森林无效，却对树模型有效？
><br>
><small>在sklearn中，决策树和随机森林都是随机选择特征进行分枝（不记得的小伙伴可以去复习第一章：决策树， 参数random_state），但决策树在建模过程中随机抽取的特征数目却远远超过随机森林当中每棵树随机抽取的特征数目（比如说对于这个780维的数据，随机森林每棵树只会抽取10~20个特征，而决策树可能会抽取 300~400个特征），因此，过滤法对随机森林无用，却对决策树有用</small>

#### 3.1.1.3 选取超参数threshold

**我们怎样知道，方差过滤掉的到底是噪音还是有效特征呢？**

每个数据集不一样，只能自己去尝试。这里的方差阈值，其实相当于是一个超参数，要选定最优的超参数，我们可以画学习曲线，找模型效果最好的点。

但现实中，我们往往不会这样去做，因为这样会耗费大量的时间。我们只会使用阈值为0或者阈值很小的方差过滤，来为我们优先消除一些明显用不到的特征，然后我们会选择更优的特征选择方法继续削减特征数量。

超参数：在开始学习过程之前设置值的参数

### 3.1.2 相关性过滤

#### 3.1.2.1 卡方过滤

卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。

>[!question]
>什么是卡方检验？
><small>卡方是衡量模型与实际观察数据的比较情况的检验，用于计算卡方的数据必须是随机的、原始的、相互排斥的、从独立变量中提取的，并且是从足够大的样本中提取的。<br>卡方检验通常用于检验假设。<br>公式是每个样本的观测值-预期值的平方除以预期值之和。<br><br>卡方检验有独立性检验和拟合优度检验。拟合优度是测试数据样本与该样本打算代表的更大群体的（已知或假定）特征的匹配程度。</small>

卡方检验类feature_selection.chi2 计算每个非负特征和标签之间的**卡方统计量**，并依照卡方统计量由高到低为特征排名。

再结合**feature_selection.SelectKBest** 这个可以输入”评分标准“来**选出前K个分数最高的特征的类**，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。有两个关键参数，第一个是模型的依赖统计量，第二个表示要选前k个。

```python
X_fschi=SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)
```

#### 3.1.2.2 选取超参数K

我们一开始是通过学习曲线去选择超参数K的，但是缺点是时间长。接下来我们就来介绍一种更好的选择k的方法：看p值选择k。

卡方检验的本质是推测两组数据之间的差异，其检验的原假设是”两组数据是相互独立的”。卡方检验返回卡方值和p值两个统计量，其中卡方值很难界定有效的范围，而p值，我们一般使用0.01或0.05作为显著性水平，即p值判断的边界，具体我们可以这样来看：
P值| <=0.05或0.01| >0.05或0.01
-|-|-
数据差异|差异不是自然形成的|这些差异是很自然的样本误差
相关性|两组数据是相关的|两组数据是相互独立的
原假设|拒绝原假设，接受备择假设|接受原假设

从特征工程的角度，**我们希望选取卡方值很大，p值小于0.05的特征**，即和标签是相关联的特征。调用 SelectKBest之前，我们可以直接从chi2实例化后的模型中获得各个特征所对应的卡方值和P值。
```python
chivalue, pvalues_chi = chi2(X_fsvar,y) 

chivalue 

pvalues_chi 

#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征： 
k = chivalue.shape[0] - (pvalues_chi > 0.05).sum() 

#X_fschi = SelectKBest(chi2, k=填写具体的k).fit_transform(X_fsvar, y) #cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()
```

观察到，所有特征的p值都是0，这说明对于digit recognizor这个数据集来说，方差验证已经把所有和标签无 关的特征都剔除了，或者这个数据集本身就不含与标签无关的特征。在这种情况下，舍弃任何一个特征，都会舍弃 对模型有用的信息，而使模型表现下降。

#### 3.1.2.3 F检验

F检验，又称ANOVA，方差齐性检验，是用来**捕捉每个特征与标签之间的线性关系**的过滤方法。它既可以做回归也可以做分类。

包含含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。

和卡方检验一样，这两个类需要和类SelectKBest连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的K。

**由于F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我 们会先将数据转换成服从正态分布的方式。**

F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，**我们希望选取p值小于0.05或0.01的特征**，这些特征与标签时显著线性相关的，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。

```python
from sklearn.feature_selection import f_classif
F, pvalues_f = f_classif(X_fsvar,y)
F

pvalues_f

k = F.shape[0] - (pvalues_f > 0.05).sum()
```
得到的结论和我们用卡方过滤得到的结论一模一样：没有任何特征的p值大于0.01，所有的特征都是和标签相关的，因此我们不需要相关性过滤。

#### 3.1.2.4 互信息法

如果存在非线性关系呢？

互信息法是用来**捕捉每个特征与标签之间的任意关系**（包括线性和非线性关系）的过滤方法。和F检验相似，它既 可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和 feature_selection.mutual_info_regression（互信息回归）。

互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。

```python
from sklearn.feature_selection import mutual_info_classif as MIC

result = MIC(X_fsvar,y)

k = result.shape[0] - sum(result <= 0)
```
所有特征的互信息量估计都大于0，因此所有特征都与标签相关。

当然了，无论是F检验还是互信息法，大家也都可以使用学习曲线，只是使用统计量的方法会更加高效。当统计量判断已经没有特征可以删除时，无论用学习曲线如何跑，删除特征都只会降低模型的表现。

如果数据量太庞大，模型太复杂，我们还是可以牺牲模型表现来提升模型速度，一切都看大家的具体需求。

### 3.1.3 过滤法总结

先方差过滤，再互信息法

类|说明|超参数的选择
-|-|-
VarianceThreshold|方差过滤，可输入方差阈值，返回方差大于阈值的新特征矩阵|看具体数据究竟是含有更多噪声还是更多有效特征<br>一般就使用0或1来筛选<br>也可以画学习曲线/取中位数跑模型帮助确认
SelectKBest|用来选取K个统计量结果最佳的特征，生成符合统计量要求的新特征矩阵|看配合使用的统计量
chi2|卡方检验，专用于分类算法，捕捉相关性|追求p小于显著性水平的特征
f_classif|F检验分类，只能捕捉线性相关性<br>要求数据服从正态分布|追求p小于显著性水平的特征
f_regression|F检验回归，只能捕捉线性相关性<br>要求数据服从正态分布|追求p小于显著性水平的特征
mutual_info_classif|互信息分类，可以捕捉任何相关性<br>不能用于稀疏矩阵|追求互信息估计大于0的特征
mutual_info_regression|互信息回归，可以捕捉任何相关性<br>不能用于稀疏矩阵|追求互信息估计大于0的特征

## 3.2 Embedded嵌入法

嵌入法是一种让算法自己决定使用哪些特征的方法，即**特征选择**和**算法训练**同时进行。在使用嵌入法时，我们先用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性，比如决策树和树的集成模型中的feature_importances_属 性，可以列出各个特征对树的建立的贡献，我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。

由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。

![[Pasted image 20230209110617.png]]

缺点：权值系数很难界定；引入算法计算缓慢。

过滤法中使用的统计量可以使用统计知识和常识来查找范围（如p值应当低于显著性水平0.05），而嵌入法中使用的权值系数却没有这样的范围可找——我们可以说，权值系数为0的特征对模型丝毫没有作用，但当大量特征都对模型有贡献且贡献不一时，我们就很难去界定一个有效的临界值。

另外，嵌入法引入了算法来挑选特征，因此其计算速度也会和应用的算法有很大的关系。如果采用计算量很大，计算缓慢的算法，嵌入法本身也会非常耗时耗力。并且，在选择完毕之后，我们还是需要自己来评估模型。

- feature_selection.SelectFromModel
*class sklearn.feature_selection.SelectFromModel (estimator, threshold=None, prefit=False, norm_order=1, max_features=None)

对于有feature_importances_的模型来说，若重要性低于提供的阈值参数，则认为这些特征不重要并被移除。 feature_importances_的取值范围是[0,1]，如果设置阈值很小，比如0.001，就可以删除那些对标签预测完全没贡 献的特征。如果设置得很接近1，可能只有一两个特征能够被留下。

对于使用惩罚项的模型来说，正则化惩罚项越大，特征在模型中对应的系数就会越小。当正则化惩罚项大到 一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋 于0。 但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择 特征系数较大的特征。

参数|说明
-|-
estimator|使用的模型评估器，只要是带feature_importances_或者coef_属性，或带有l1和l2惩罚 项的模型都可以使用
threshold|特征重要性的阈值，重要性低于这个阈值的特征都将被删除
prefit|默认False，判断是否将实例化后的模型直接传递给构造函数。如果为True，则必须直接 调用fit和transform，不能使用fit_transform，并且SelectFromModel不能与 cross_val_score，GridSearchCV和克隆估计器的类似实用程序一起使用。
norm_order|k可输入非零整数，正无穷，负无穷，默认值为1<br>在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数
max_features|在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据max_features选择，请设置 threshold = -np.inf

怎么取合适的threshold？学习曲线！

np.linspace是用来选取最大值最小值中间有限个数的。

```python
X_embedded = SelectFromModel(RFC_,threshold=0.00067).fit_transform(X,y) X_embedded.shape

cross_val_score(RFC_,X_embedded,y,cv=5).mean()
```

特征个数瞬间缩小到324多，这比我们在方差过滤的时候选择中位数过滤出来的结果392列要小，并且 交叉验证分数0.9399高于方差过滤后的结果0.9388。

比起要思考很多统计量的过滤法来说，嵌入法可能是更有效的一种方法。然而，在算法本身很复杂的时候，过滤法的计算远远比嵌入法要快，所以大型数据中，我们还是会优先考虑过滤法。

## 3.3 Wrapper包装法


包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如 coef_属性或feature_importances_属性来完成特征选择。

但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。

包装法在初始特征集上训练评估器，并且通过 coef_属性或通过feature_importances_属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的 特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。
包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。

![[Pasted image 20230209115219.png]]

算法：专业的数据挖掘算法，就是我们的目标函数，核心功能是选取最佳特征子集。

最典型的目标函数是**递归特征消除法**（Recursive feature elimination, 简写为RFE）。

- feature_selection.RFE
*class sklearn.feature_selection.RFE (estimator, n_features_to_select=None, step=1, verbose=0)

参数estimator是需要填写的实例化后的评估器，n_features_to_select是想要选择的特征个数，step表示每次迭代中希望移除的特征个数。

除此之外，RFE类有两个很重要的属性，.support_：返回所有的特征的是否最后被选中的布尔矩阵，以及.ranking_返回特征的按数次迭代中综合重要性的排名。

类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。

## 3.4 特征选择总结

经验来说，过滤法更快速，但更粗糙。包装法和嵌入法更精确，比较适合具体到算法去调整，但计算量比较大，运行时间长。

当数据量很大的时候，优先使用方差过滤和互信息法调整，再上其他特征选择方法。

使用逻辑回归时，优先使用嵌入法。
使用支持向量机时，优先使用包装法。
迷茫的时候，从过滤法走起，看具体数据具体分析。

真正的高手，往往使用特征创造或特征提取来寻找高级特征。