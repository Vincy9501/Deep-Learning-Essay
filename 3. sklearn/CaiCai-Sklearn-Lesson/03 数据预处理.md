# 1. 概述

## 1.1 数据挖掘五大流程

1. 获取数据
2. 数据预处理
3. 特征工程
4. 建模
5. 上限

## 1.2 sklearn中的数据预处理和特征工程

- 模块preprocessing：几乎包含数据预处理的所有内容
- 模块Impute：填补缺失值专用 
- 模块feature_selection：包含特征选择的各种方法的实践
- 模块decomposition：包含降维算法

# 2. 数据预处理

## 2.1 数据无量纲化

无量纲化：将不同规格的数据转换到同一规格/不同分布的数据转换到某个特定分布

- 分类
	- 线性：中心化处理/缩放处理
		- 中心化：让所有记录减去一个固定值，即让数据样本平移到某个位置
		- 缩放：通过除以一个固定值，将数据固定到某个范围中
	- 非线性

### 2.1.1 preprocessing.MinMaxScaler

**归一化**（中心化+缩放）：数据按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间

$$x^{*} = \frac {x-min(x)}{max(x)-min(x)}$$
代码：
```python
from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]

import pandas as pd 
pd.DataFrame(data)

#实现归一化 
scaler = MinMaxScaler() #实例化 
scaler = scaler.fit(data) #fit，在这里本质是生成min(x)和max(x) 
result = scaler.transform(data) #通过接口导出结果
result

result_ = scaler.fit_transform(data) #训练和导出结果一步达成
scaler.inverse_transform(result) #将归一化后的结果逆转（还原矩阵）

data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] 
scaler = MinMaxScaler(feature_range=[5,10]) #依然实例化
result = scaler.fit_transform(data) #fit_transform一步导出结果 
result

#当X中的特征数量非常多的时候，fit会报错并表示，数据量太大了我计算不了 
#此时使用partial_fit作为训练接口 
#scaler = scaler.partial_fit(data)
```

BONUS: 使用numpy来实现归一化

```python
import numpy as np X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]]) #归一化 
X_nor = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) X_nor 
#逆转归一化 
X_returned = X_nor * (X.max(axis=0) - X.min(axis=0)) + X.min(axis=0) X_returned
```

### 2.1.2 preprocessing.StandardScaler

当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布），而这个过程，就叫做当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分 布），而这个过程，就叫做数据标准化(Standardization，又称Z-score normalization)，公式如下：(Standardization，又称Z-score normalization)，公式如下：
$$x^{*} = \frac {x-\mu}{\sigma}$$
```python
from sklearn.preprocessing import StandardScaler 
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]

scaler = StandardScaler() #实例化 
scaler.fit(data) #fit，本质是生成均值和方差

scaler.mean_ #查看均值的属性mean_ 
scaler.var_ #查看方差的属性var_

x_std = scaler.transform(data) #通过接口导出结果

x_std.mean() #导出的结果是一个数组，用mean()查看均值 
x_std.std() #用std()查看方差

scaler.fit_transform(data) #使用fit_transform(data)一步达成结果

scaler.inverse_transform(x_std) #使用inverse_transform逆转标准化
```

对于StandardScaler和MinMaxScaler来说，空值NaN会被当做是缺失值，在fit的时候忽略，在transform的时候保持缺失NaN的状态显示。

### 2.1.3 StandardScaler和MinMaxScaler怎么选？

看情况。大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。

MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像 处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。

建议先试试看StandardScaler，效果不好换MinMaxScaler。

## 2.2 缺失值

### 2.2.1 impute.SimpleImputer

*class sklearn.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True)

这个类是专门用来填补缺失值的。它包括四个重要参数：
参数|含义&输入
-|-
missing_values |告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan 
strategy |我们填补缺失值的策略，默认均值。 <br>输入“mean”使用均值填补（仅对数值型特征可用） <br>输入“median"用中值填补（仅对数值型特征可用） <br>输入"most_frequent”用众数填补（对数值型和字符型特征都可用） <br>输入“constant"表示请参考参数“fill_value"中的值（对数值型和字符型特征都可用） 
fill_value|当参数startegy为”constant"的时候可用，可输入字符串或数字表示要填充的值，常用0 
copy |默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去。

## 2.3 处理分类型特征：编码与哑变量

大多数的算法，逻辑回归、支持向量机只能够处理数值型数据，不能处理文字，在sklearn中，除了用来处理文字的算法，其他算法在fit的时候要求全部输入数组或矩阵，不能导入文字型数据。

决策树和朴素贝叶斯可以处理文字，但是sklearn中规定必须导入数值型。我们必须将数据进行编码，即是说，将文字型数据转换为数值型。

### 2.3.1 preprocessing.LabelEncoder

将分类转换为分类数值

```python
from sklearn.preprocessing import LabelEncoder

y = data.iloc[:,-1] #要输入的是标签，不是特征矩阵，所以允许一维

le = LabelEncoder() #实例化 
le = le.fit(y) #导入数据 
label = le.transform(y)   #transform接口调取结果

le.classes_ #属性.classes_查看标签中究竟有多少类别 
label #查看获取的结果label

le.fit_transform(y) #也可以直接fit_transform一步
le.inverse_transform(label) #使用inverse_transform可以逆转

data.iloc[:,-1] = label #让标签等于我们运行出来的结果
data.head()

#如果不需要教学展示的话我会这么写： 
from sklearn.preprocessing import LabelEncoder 
data.iloc[:,-1] = LabelEncoder().fit_transform(data.iloc[:,-1])

```

### 2.3.2 preprocessing.OrdinalEncoder

将分类特征转换为分类数值

```python
from sklearn.preprocessing import OrdinalEncoder

#接口categories_对应LabelEncoder的接口classes_，一模一样的功能 
data_ = data.copy()

data_.head()

OrdinalEncoder().fit(data_.iloc[:,1:-1]).categories_

data_.iloc[:,1:-1] = OrdinalEncoder().fit_transform(data_.iloc[:,1:-1])

data_.head()
```

### 问题：Label Encoding 和 Ordinal Encoding的区别

- 两者具有相同的功能。有点不同的是背后的想法。`OrdinalEncoder`用于转换特征，而`LabelEncoder`用于转换目标变量。
- `OrdinalEncoder`用于具有形状的二维数据`(n_samples, n_features)`
- `LabelEncoder`适用于具有形状的一维数据`(n_samples,)`

### 2.3.3 preprocessing.OneHotEncoder

独热编码，创建哑变量（简单来说就是把多分类变量转换为二分变量的一种形式）

>[!question]
>什么是哑变量？
><small>虚拟变量 ( Dummy Variables) 又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。</small>

我们来思考三种不同性质的分类数据：
1.  舱门（S，C，Q） 
三种取值S，C，Q是相互独立的，彼此之间完全没有联系，表达的是S≠C≠Q的概念。这是名义变量。
2. 学历（小学，初中，高中）
在性质上可以有高中>初中>小学这样的联系，但是学 历取值之间却不是可以计算的，这是有序变量。
3. 体重（>45kg，>90kg，>135kg）
各个取值之间有联系，且是可以互相计算的，比如120kg - 45kg = 90kg，分类之间可以通过数学计算互相转换。是有距变量。

类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息：

![[Pasted image 20230208165404.png]]

```python
enc.get_feature_names() #返回每一个稀疏矩阵列的名字
```

## 2.4 处理连续型特征：二值化与分段

根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。

默认阈值为0时，特征中所有的正值都映射到1。

二值化是对文本计数数据的常见操作。

### 2.4.1 preprocessing.Binarizer

```python
data_2 = data.copy()

from sklearn.preprocessing import Binarizer X = data_2.iloc[:,0].values.reshape(-1,1) #类为特征专用，所以不能使用一维数组 
transformer = Binarizer(threshold=30).fit_transform(X)

transformer
```

### 2.4.2 preprocessing.KBinsDiscretizer

这是将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。

参数|含义&输入 
-|-
n_bins|每个特征中分箱的个数，默认5，一次会被运用到所有导入的特征
encode|编码的方式，默认“onehot” <br>"onehot"：做哑变量，之后返回一个稀疏矩阵，每一列是一个特征中的一个类别，含有该类别的样本表示为1，不含的表示为0 <br>“ordinal”：每个特征的每个箱都被编码为一个整数，返回每一列是一个特征，每个特征下含 有不同整数编码的箱的矩阵 <br>"onehot-dense"：做哑变量，之后返回一个密集数组。
strategy|用来定义箱宽的方式，默认"quantile"<br>"uniform"：表示等宽分箱，即每个特征中的每个箱的最大值之间的差为 (特征.max() - 特征.min())/(n_bins)<br>"quantile"：表示等位分箱，即每个特征中的每个箱内的样本数量都相同<br>"kmeans"：表示按聚类分箱，每个箱中的值到最近的一维k均值聚类的簇心得距离都相同

# 3. 特征选择 feature_selection

特征提取 (feature extraction) | 特征创造 (feature creation)|特征选择 (feature selection)
-|-|-
从文字，图像，声音等其他非结构化数据中提取新信息作为特征。比如说，从淘宝宝贝的名称中提取出 产品类别，产品颜色，是否是网红产品等等。|把现有特征进行组合，或互相计算，得到新的特征。比如说，我们有一列特征是速度，一列特征是距离，我们就可以通过让两列相处，创造新的特征：通过距离所花的时间。|从所有的特征中，选择出有意义，对模型有帮助的特征，以避免必须将所有特征都导入模型 去训练的情况。

特征工程第一步：理解业务

## 3.1 Filter过滤法

过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法。

它是根据各种统计检验中的分数以及相关性的各项指标来选择特征。

流程：

**全部特征 --> 最佳特征子集 --> 算法 --> 模型评估**

### 3.1.1 方差过滤

#### 3.1.1.1 VarianceThreshold

通过特征本身的方差筛选特征的类，如果方差很小那就说明样本在这个特征上没什么差异，就对样本区分没什么作用。

threshold表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0。

```python
from sklearn.feature_selection import VarianceThreshold 
selector = VarianceThreshold() #实例化，不填参数默认方差为0 
X_var0 = selector.fit_transform(X) #获取删除不合格特征之后的新特征矩阵
```

如果想留下一半特征：
```python
import numpy as np X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
```

当特征是二分类的时候，特征的取值就是伯努利随机变量，这些变量的方差可以计算为：
$$Var[X] = p(1-p)$$
#### 3.1.1.2 方差过滤对模型的影响

KNN和随机森林在方差过滤上有不同的表现，在多特征的情况下，KNN运行时间远远大于随机森林。这是因为随机森林只会选取固定数量的特征建模。

因此：
过滤法的对象：需要遍历特征或升维的算法
过滤法的目标：在维持算法表现的前提下，帮助算法们降低计算成本

KNN：是K近邻算法中的分类算法，其原理非常简单，是利用每个样本到 其他样本点的距离来判断每个样本点的相似度，然后对样本进行分类。

>[!question]
>过滤法对随机森林无效，却对树模型有效？
><br>
><small>在sklearn中，决策树和随机森林都是随机选择特征进行分枝（不记得的小伙伴可以去复习第一章：决策树， 参数random_state），但决策树在建模过程中随机抽取的特征数目却远远超过随机森林当中每棵树随机抽取的特征数目（比如说对于这个780维的数据，随机森林每棵树只会抽取10~20个特征，而决策树可能会抽取 300~400个特征），因此，过滤法对随机森林无用，却对决策树有用</small>

#### 3.1.1.3 选取超参数threshold

**我们怎样知道，方差过滤掉的到底时噪音还是有效特征呢？**

每个数据集不一样，只能自己去尝试。这里的方差阈值，其实相当于是一个超参数，要选定最优的超参数，我们可以画学习曲线，找模型效果最好的点。

但现实中，我们往往不会这样去做，因为这样会耗费大量的时间。我们只会使用阈值为0或者阈值很小的方差过滤，来为我们优先消除一些明显用不到的特征，然后我们会选择更优的特征选择方法继续削减特征数量。

超参数：在开始学习过程之前设置值的参数

### 3.1.2 卡方过滤

卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。

卡方检验类feature_selection.chi2 计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。

再结合feature_selection.SelectKBest 这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。