# 1. 概述

## 1.1 数据挖掘五大流程

1. 获取数据
2. 数据预处理
3. 特征工程
4. 建模
5. 上限

## 1.2 sklearn中的数据预处理和特征工程

- 模块preprocessing：几乎包含数据预处理的所有内容模
- 模块Impute：填补缺失值专用 
- 模块feature_selection：包含特征选择的各种方法的实践
- 模块decomposition：包含降维算法

# 2. 数据预处理

## 2.1 数据无量纲化

无量纲化：将不同规格的数据转换到同一规格/不同分布的数据转换到某个特定分布

- 分类
	- 线性：中心化处理/缩放处理
		- 中心化：让所有记录减去一个固定值，即让数据样本平移到某个位置
		- 缩放：通过除以一个固定值，将数据固定到某个范围中
	- 非线性

### 2.1.1 preprocessing.MinMaxScaler

**归一化**（中心化+缩放）：数据按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间

$$x^{*} = \frac {x-min(x)}{max(x)-min(x)}$$
代码：
```python
from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]

import pandas as pd 
pd.DataFrame(data)

#实现归一化 
scaler = MinMaxScaler() #实例化 
scaler = scaler.fit(data) #fit，在这里本质是生成min(x)和max(x) 
result = scaler.transform(data) #通过接口导出结果
result

result_ = scaler.fit_transform(data) #训练和导出结果一步达成
scaler.inverse_transform(result) #将归一化后的结果逆转（还原矩阵）

data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] 
scaler = MinMaxScaler(feature_range=[5,10]) #依然实例化
result = scaler.fit_transform(data) #fit_transform一步导出结果 
result

#当X中的特征数量非常多的时候，fit会报错并表示，数据量太大了我计算不了 
#此时使用partial_fit作为训练接口 
#scaler = scaler.partial_fit(data)
```

BONUS: 使用numpy来实现归一化

```python
import numpy as np X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]]) #归一化 
X_nor = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) X_nor 
#逆转归一化 
X_returned = X_nor * (X.max(axis=0) - X.min(axis=0)) + X.min(axis=0) X_returned
```

### 2.1.2 preprocessing.StandardScaler

当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布），而这个过程，就叫做当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分 布），而这个过程，就叫做数据标准化(Standardization，又称Z-score normalization)，公式如下：(Standardization，又称Z-score normalization)，公式如下：
$$x^{*} = \frac {x-\mu}{\sigma}$$
```python
from sklearn.preprocessing import StandardScaler 
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]

scaler = StandardScaler() #实例化 
scaler.fit(data) #fit，本质是生成均值和方差

scaler.mean_ #查看均值的属性mean_ 
scaler.var_ #查看方差的属性var_

x_std = scaler.transform(data) #通过接口导出结果

x_std.mean() #导出的结果是一个数组，用mean()查看均值 
x_std.std() #用std()查看方差

scaler.fit_transform(data) #使用fit_transform(data)一步达成结果

scaler.inverse_transform(x_std) #使用inverse_transform逆转标准化
```

对于StandardScaler和MinMaxScaler来说，空值NaN会被当做是缺失值，在fit的时候忽略，在transform的时候保持缺失NaN的状态显示。

### 2.1.3 StandardScaler和MinMaxScaler怎么选？

看情况。大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。

MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像 处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。

建议先试试看StandardScaler，效果不好换MinMaxScaler。

## 2.2 缺失值

### 2.2.1 impute.SimpleImputer

*class sklearn.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True)

这个类是专门用来填补缺失值的。它包括四个重要参数：
参数|含义&输入
-|-
missing_values |告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan 
strategy |我们填补缺失值的策略，默认均值。 <br>输入“mean”使用均值填补（仅对数值型特征可用） <br>输入“median"用中值填补（仅对数值型特征可用） <br>输入"most_frequent”用众数填补（对数值型和字符型特征都可用） <br>输入“constant"表示请参考参数“fill_value"中的值（对数值型和字符型特征都可用） 
fill_value|当参数startegy为”constant"的时候可用，可输入字符串或数字表示要填充的值，常用0 
copy |默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去。
