# 1. 安装yolov5

在github右侧Releases选择版本，找到所需的v6.1版本。

在Asset中找到自己需要的与训练权重文件。

# 2. 环境配置

## 2.1 确定需求

找到*requirements.txt*文件，写了需要配置的环境版本。
大部分可以用`pip install -r requirements.txt`直接安装。
但是torch和torchversion需要去[官网](https://pytorch.org/get-started/previous-versions/)确认对应版本。

```txt
# CUDA 11.0
conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch

# WHEEL
pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
```

## 2.2 安装CUDA cuDNN

装好了，这步省略

## 2.3 创建虚拟环境

```cmd
conda create -n pytorch python=3.6
conda activate pytorch
(pytorch) C:\Users>pip list # 查看已经安装的包
(pytorch) C:\Users>pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
```

验证：
```txt
python
import torch
print(torch.cuda.is_available())
print(torch.__version__)
```

其他见[[#2.1 确定需求]]。

# 3. 下载权重文件

放在项目文件夹中。

# 4. 自制VOC2007数据集

## 4.1 文件用途

VOC2007/ImageSets/Main文件夹里面存放 test.txt train.txt trainval.txt val.txt四个文件。

train.txt 是用来训练的图片文件的文件名列表 （训练集）
val.txt是用来验证的图片文件的文件名列表 （验证集）
trianval.txt是用来训练和验证的图片文件的文件名列表
test.txt 是用来测试的图片文件的文件名列表 （测试集）

- train是网络模型在训练的时候用的，而val是网络模型在训练过程中测试用的。val是不影响训练的。
- 在训练的时候可以得到train和val这两个数据集的误差率，利用这个误差率可以绘制出学习曲线，通过观察学习曲线，可以发现一些网络模型的问题，然后再根据这些问题去调整网络参数。
- test就是网络模型训练完毕测试用的。

## 4.2 数据集准备

训练前将标签文件放在VOCdevkit文件夹下的VOC2007文件夹下的Annotation中。
训练前将图片文件放在VOCdevkit文件夹下的VOC2007文件夹下的JPEGImages中。

## 4.3 数据集的处理

在完成数据集的摆放之后，我们需要对数据集进行下一步的处理，目的是获得训练用的2007_train.txt以及2007_val.txt，需要用到根目录下的voc_annotation.py。

### 4.3.1 voc_annotation.py里面有一些参数需要设置
分别是annotation_mode、classes_path、trainval_percent、train_percent、VOCdevkit_path，第一次训练可以仅修改classes_path。

```python
'''
annotation_mode用于指定该文件运行时计算的内容
annotation_mode为0代表整个标签处理过程，包括获得VOCdevkit/VOC2007/ImageSets里面的txt以及训练用的2007_train.txt、2007_val.txt
annotation_mode为1代表获得VOCdevkit/VOC2007/ImageSets里面的txt
annotation_mode为2代表获得训练用的2007_train.txt、2007_val.txt
'''
annotation_mode     = 0
'''
必须要修改，用于生成2007_train.txt、2007_val.txt的目标信息
与训练和预测所用的classes_path一致即可
如果生成的2007_train.txt里面没有目标信息
那么就是因为classes没有设定正确
仅在annotation_mode为0和2的时候有效
'''
classes_path        = 'model_data/voc_classes.txt'
'''
trainval_percent用于指定(训练集+验证集)与测试集的比例，默认情况下 (训练集+验证集):测试集 = 9:1
train_percent用于指定(训练集+验证集)中训练集与验证集的比例，默认情况下 训练集:验证集 = 9:1
仅在annotation_mode为0和1的时候有效
'''
trainval_percent    = 0.9
train_percent       = 0.9
'''
指向VOC数据集所在的文件夹
默认指向根目录下的VOC数据集
'''
VOCdevkit_path  = 'VOCdevkit'
```

### 4.3.2 xml_2_txt.py，生成labels文件夹和三个.txt文档
```python
import xml.etree.ElementTree as ET
import pickle
import os
from os import listdir, getcwd
from os.path import join
 
#sets设置的就是
sets=['train', 'val', 'test']
 
 
# classes = ["aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
classes = ["drone"]  # 修改为自己的label
 
def convert(size, box):
    dw = 1./(size[0])  # 有的人运行这个脚本可能报错，说不能除以0什么的，你可以变成dw = 1./((size[0])+0.1)
    dh = 1./(size[1])  # 有的人运行这个脚本可能报错，说不能除以0什么的，你可以变成dh = 1./((size[0])+0.1)
    x = (box[0] + box[1])/2.0 - 1
    y = (box[2] + box[3])/2.0 - 1
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)
 
def convert_annotation(image_id):
    in_file = open('VOCdevkit/VOC2007/Annotations/%s.xml'%(image_id))
    out_file = open('VOCdevkit/VOC2007/labels/%s.txt'%(image_id), 'w')
    tree=ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)
 
    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult)==1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))
        bb = convert((w,h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')
 
wd = getcwd()
 
for image_set in sets:
    if not os.path.exists('VOCdevkit/VOC2007/labels/'):  # 修改路径（最好使用全路径）
        os.makedirs('VOCdevkit/VOC2007/labels/')  # 修改路径（最好使用全路径）
    image_ids = open('VOCdevkit/VOC2007/ImageSets/Main/%s.txt' % (image_set)).read().strip().split()  # 修改路径（最好使用全路径）
    list_file = open('VOCdevkit/VOC2007/%s.txt' % (image_set), 'w')  # 修改路径（最好使用全路径）
    for image_id in image_ids:
        list_file.write('VOCdevkit/VOC2007/JPEGImages/%s.jpg\n' % (image_id))  # 修改路径（最好使用全路径）
        convert_annotation(image_id)
    list_file.close()
 
```

### 4.3.3 建立自己的yaml
```python
train: VOCdevkit/VOC2007/train.txt #此处是xml_2_txt.py生成的train.txt的路径，不要弄成Main文件夹下的.txt
val: VOCdevkit/VOC2007/val.txt #此处是xml_2_txt.py生成的train.txt的路径，不要弄成Main文件夹下的.txt
test: VOCdevkit/VOC2007/test.txt #此处是xml_2_txt.py生成的train.txt的路径，不要弄成Main文件夹下的.txt
 
# Classes
nc: 1  # number of classes 数据集类别数量
names: ['drone']  # class names 数据集类别名称，注意和标签的顺序对应
```

### 4.3.4 改动datasets.py中的代码

`sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep`，改成
`sa, sb = os.sep + 'JPEGImages' + os.sep, os.sep + 'labels' + os.sep`

```python
def img2label_paths(img_paths):
    # Define label paths as a function of image paths
    sa, sb = os.sep + 'JPEGImages' + os.sep, os.sep + 'labels' + os.sep  # /images/, /labels/ substrings
    return [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths]
```

### 4.3.5 在所用模型的.yaml中将nc类别数量改掉

### 4.3.6 train.py中把路径对应好，运行即可开始训练

```python
def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default='weights/yolov5s.pt', help='initial weights path')
    parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')
    parser.add_argument('--data', type=str, default='data/try.yaml', help='datasets.yaml path')
```

## 4.4 开始训练


# 报错记录

## 1. 缺少字体Arial.ttf

`Downloading https://ultralytics.com/assets/Arial.ttf to /data/..../.config/Ultralytics/Arial.ttf_downloading arial.ttf_ZGPing`

解决方法：下载字体并保存在`/data/..../.config/Ultralytics/`路径下。

## 2. loss部分，输出类型问题

`RuntimeError: result type Float can‘t be cast to the desired output type long int`

解决方法：
1. 打开你的【utils】文件下的【loss.py】
2. 【Ctrl】+【F】打开搜索功能，输入【for i in range(self.nl)】: 
```python
for i in range(self.nl):
	# anchors = self.anchors[i]
	anchors, shape = self.anchors[i], p[i].shape
	gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]] # xyxy gain
```
3. 按【Ctrl】+【F】打开搜索功能，输入【indices.append】找到下面的一行内容：
```python
# Append
a = t[:, 6].long() # anchor indices
# indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))
indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid
```

还有第二种解决方法：

找到gain = torch.ones(7, device=targets.device)，将其修改为gain = torch.ones(7, device=targets.device).long()

## 3. 断点续训

1. train.py文件中找到函数parse_opt，修改参数–resume的默认参数为Ture

```python
parser.add_argument('--resume', nargs='?', const=True, default=True, help='resume most recent training')
```

2. 找到上次训练最后的权重

runs/train/exp*/weights/路径下找到上次训练的最后的保存的权重，加载[断点](https://so.csdn.net/so/search?q=%E6%96%AD%E7%82%B9&spm=1001.2101.3001.7020)之前最后保留的模型

3. 命令行输入

```python
python train.py --data ./data/mchar.yaml --cfg yolov5s_mchar.yaml --epochs 8 --batch-size 8 --weights ./runs/train/exp7/weights/last.pt
```

## 4. OSError [WinError 1455] 页面文件太小,无法完成操作

在dataset.py文件中，可以搜索到num_workers变量，`num_workers = nw`，修改为2，解决这个报错问题。

> num_workers即工作进程数，在dataloader加载数据时，num_workers可以看作搬砖的工人，将batch加载进RAM，工人越多加载速度越快。一般这个数量设置值是自己电脑/服务器的CPU核心数。
> 如果num_worker设为0，意味着每一轮迭代时，dataloader不再有自主加载数据到RAM这一步骤（因为没有worker了），而是在RAM中找batch，找不到时再加载相应的batch。

- `num_worker`设置得大，好处是寻batch速度快，因为下一轮迭代的batch很可能在上一轮/上上一轮...迭代时已经加载好了。坏处是内存开销大，也加重了CPU负担（worker加载数据到RAM的进程是CPU复制的嘛）。
- `num_workers`的经验设置值是自己电脑/服务器的CPU核心数，如果CPU很强、RAM也很充足，就可以设置得更大些。

## 5. 训练过程中想减少或者增加epoch

1. 首先要Ctrl + C中止训练，然后到train.py，修改parse_opt函数中的‘–resume’ 将default=False改为default=True
2. 修改opy.yaml中数最大的那个里面的参数。
	> 继续进行训练时，你还按照刚开始训练时设置参数就不好用了，因为训练参数已经写进了opt.yaml文件里，这个文件的位置在 ./runs/train/exp
3. 运行train.py

## 6. 为什么进行冻结训练？

在冻结阶段，模型的主干被冻结了，特征提取网络不发生改变。占用的显存较小，仅对网络进行微调。  
在解冻阶段，模型的主干不被冻结了，特征提取网络会发生改变。占用的显存较大，网络所有的参数都会发生改变。

## 7. 训练yolov5时对workers和batch-size的理解

- **workers**: 指数据装载时cpu所使用的线程数，默认为8
- **batch-size**: 就是一次往GPU哪里塞多少张图片了。决定了显存占用大小，默认是16。

- workers需要根据显卡算力调整，太大时gpu处理不过来，虚拟内存（磁盘空间）会成倍占用；如果设置得太小，gpu会跑不满。
> 为2比较好

- batch-size：当为8的倍数时效率更高一点

## 8. 训练速度慢的情况和解决方法

### 8.1 GPU 太差

要么降低batchsize和worker，还有imgsize，要么就换一张显卡。
三个参数的设置都在train.py里。

### 8.2 完全是CPU在跑

看看显存是否被占用或者有没有安装pytorch和cuda。
```python
import torch
 
print(torch.__version__)
torch.cuda.is_available()
```

### 8.3 性能过剩

```python
parser.add_argument('--cache', action='store_false', help='--cache images in "ram" (default) or "disk"')
```

没用

1. 7：3划分训练集验证集，分好之后放在snd_sanjiao中，label中要有classes文件
2. anaconda环境变量
3. ![[Pasted image 20230301101558.png]]
project存放训练结果