# 1. 数据和架构

本文中使用的数据集包含三个特征，目标类只有两个值——`1`通过和`0`失败。目标是将数据点分类为两个类别中的任何一个——二进制分类的一个例子。

![[Pasted image 20230201170941.png]]

**理解**：正向传播允许信息沿一个方向流动——从输入层到输出层，而反向传播则相反——允许数据从输出向后流动，同时更新参数（权重和偏差）。

**定义**：反向传播是一种监督学习的方法，**神经网络用来更新参数**，使网络的预测更加准确。参数优化过程是使用称为梯度下降的优化算法实现的（这个概念在您继续阅读时会非常清楚）。

前向传播产生一个预测值${yhat}$，其与目标值y有一个损失值，这个损失值被成本函数捕获：

![[Pasted image 20230201171141.png|350]]

m是训练示例的数量，L是误差，我们的目标是最小化成本E。这是通过对(wrt)参数(权重和参数)微分E并在梯度的相反方向调整参数来实现的。

由于在这个实例中只考虑1个训练示例(m=1)上的反向传播，因此E，简化为：

![[Pasted image 20230201171551.png|375]]

# 2. 选择损失函数L

对于分类问题，交叉熵（也称为对数损失）和铰链损失是合适的损失函数，而均方误差 (MSE) 和平均绝对误差 (MAE) 是回归任务的合适损失函数。

二元交叉熵损失函数适用于我们的二元分类任务——数据有两个类别，`0`或者`1`。

![[Pasted image 20230201171715.png|475]]

t =1是真值标签，yhat=0.521是模型的输出，ln是以2为底的自然对数。

# 3. 数据和参数

下表显示了3-4-1神经网络各层的数据。在输出层显示的值来自我们提供给模型用于训练的数据。第二层/隐藏层包含我们希望更新的权重(w)和偏差(b)，以及向前传递期间4个神经元的每个神经元的输出(f)。输出包含参数(w和b)和模型的输出(yhat)——这个值实际上是模型训练每次迭代时的模型预测。在单次正向传递后，yhat=0.521。

![[Pasted image 20230201171905.png]]
- i - 是前一层的节点，$l-1$
- j - 当前层$l$

## 3.1 更新方程和损失函数

E是y、yhat的函数，yhat是w和b的函数。x是数据，g是激活函数。因此，E是w和b的函数，因此可以根据这些参数进行微分：

![[Pasted image 20230201172405.png|475]]

$t$是当前步骤， $\epsilon$是学习率，它决定了权重和偏差的更新速率，我们将使用0.5。
因此等式变为：
![[Pasted image 20230201172721.png|475]]

由于我们正在处理二进制分类，因此我们将使用定义为的二进制交叉熵损失函数：

![[Pasted image 20230201172805.png]]

在所有层中使用sigmoid激活函数：

![[Pasted image 20230201172825.png|400]]

$z = wx + b$

## 3.2 更新隐藏层的参数

与正向传播不同，反向传播从输出层向后传播到层`1`。我们需要针对所有层的参数计算导数/梯度。为此，我们必须了解**微分的链式法则**。

我们的目的是了解w和b怎样影响E，而已知$$E = L(y, yhat), yhat = g(z), z = wx + b$$
因此：

### 3.2.1 计算权重的导数

根据微分链式法则，有：

![[Pasted image 20230201173018.png]]

接下来是




