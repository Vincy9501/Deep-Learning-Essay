# 1. 梯度下降概念

梯度下降是一种数学优化的手段，目标是找到最小化损失的最佳参数，将这些参数视为试图下降到山谷中的最低点的最优参数。

## 1.1 梯度

微积分中对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。

<small>比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。</small>

它代表了**函数变化增加最快的地方**，这意味着对于函数f(x,y)，在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)T的方向是f(x,y)增加最快的地方，**沿着梯度向量的方向容易找到最大值，沿着相反方向容易找到最小值**。

## 1.2 损失函数

><small>损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示。通常可以表示成如下式子：</small>
![[Pasted image 20230130182708.png]]
><small>其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示找到使目标函数最小时的θ值。</small>

在机器学习算法中，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。

# 2. 原理

假设我们在一座山上，此时需要寻找下山方向，由于梯度代表了**函数变化增加最快的地方**，我们可以每走一步算一次梯度。这个决策会使我们获得全局最优解或者局部最优解。

![[Pasted image 20230130183511.png]]

## 2.1 相关概念

1. 步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。
2. 特征（feature）：指的是样本中输入部分，比如2个单特征的样本$（x^{(0)},y^{(0)}）$,$（x^{(1)},y^{(1)}）$,则第一个样本特征为$x^{(0)}$ ，第一个样本输出为$y^{(0)}$ 。
3. 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ^{(x)}$ 。
4. 损失函数（loss function）：损失函数小，意味着拟合程度好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。

## 2.2 引入

假设给定了三个可以在二维平面上表示的数字 (1,1)、(2,2)、(3,3)，如图所示：

![[Pasted image 20230130202002.png]]

现在，有人要求您在尽可能靠近所有可用点的地方画一条线，这些线遵循的等式看起来像这样：
			$$h(x) = θ_0 + θ_1 x$$
假设我们想出了三种不同类型的线，具有三种不同的斜率和截距值，如下所示。该表有上线、中线和下线的$θ_0$和上线、中线和下线的$θ_1$。
![[Pasted image 20230130202422.png]]

### 成本函数

成本函数基本上意味着你的预测线与我们已经给出的实际点有多远。


>[!question]
>这里的成本函数和损失函数有什么区别？

loss近似等于cost, 但cost针对多个样本，而loss仅针对单个样本。

所以现在计算原始点和预测线的距离，可以使用如下公式：
			$$J(θ_0,θ_1) = \frac{1}{2m}\sum^m_{1}(h(x^{(i)}) - y^{(i)})^2$$
第一项 $\frac{1}{2m}$是常数项，其中 m 表示我们已经拥有的数据点的数量。2是为了方便后面求导，其实是可以取任意实数。$h(x^{(i)})$表示我们对 i 的特定值的假设的输出。例如，在例子中，原始点的值为 (1,1)、(2,2)、(3,3)。
因此对于中间线：
$$\frac{1}{2*3}(1-1 + 2-2 + 3-3)^2 = 0$$

### 降低成本函数

上面预测了 theta-0 和 theta-1 的值，因为它们只是预测，只有中间的一个是完美的。但在现实生活中，找到 theta-0 和 theta-1 的完美值几乎是不可能的。但是，对于任何给定的一组值都可以找到具有最低成本值的线。

因为我们知道改变 theta-0 和 theta-1 的值，线的方向可以改变，为了达到一条尽可能接近这三个点的线，减少了所有 theta 的值一点一点地达到成本函数的最小值。
$$θ_j := θ_j - α \frac{∂}{∂θ}J(θ_0,θ_1)$$
α表示正在以多少幅度降低值。这里的 Theta-j 代表解决方案中的每个单独的 theta，对所有的 theta 运行这个方程，在我们的例子中是两个，但也可能更多。

然后，用这里的公式，把所有的theta减少α的幅度，然后沿着预测的直线向下移动一点。

然后再次运行该公式，减少 thetas 的值，查看该线的样子，计算成本并为下一次迭代做好准备。

然后再次减少 theta 的值，再次查看这条线并计算成本。直到达到成本最低的点。

综上可得：
$$θ_j := θ_j - α \frac{1}{m}\sum^m_{1}(h(x^{(i)}) - y^{(i)})x^{(i)}$$

### 概括

1. 获得一些数据点；
2. 使用它们计算 thetas 的值并使用假设方程绘制图形；
3. 使用成本函数计算成本；
4. 使用梯度下降将 thetas 的值减少 alpha 的幅度；
5. 使用新的一组 thetas 值，可以再次计算成本；
6. 不断重复直到达到成本函数的最小值。


## 2.3 详细算法


1. 确定假设函数
			$$h_θ(x_1, x_2, ... x_n) = θ_0 + θ_1 x_1 + ... + θ_n x_n$$
	$θ_i(i = 0, 1, 2 ... n)$为模型参数
	$x_i(i = 0, 1, 2 ... n)$为每个样本的n个特征值

	增加一个特征值$x_0 = 1$，这样：
	$$h_θ(x_0, x_1, ... x_n) = \sum^n_{i=0}θ_i x_i$$
2. 确定损失函数
			$$J(θ_0, θ_1, ... θ_n) = \frac{1}{2m}\sum^m_{j=1}(h_θ(x_0^{(j)}, x_1^{(j)}, ... x_n^{(j)}) - y_j)^2$$
3. 相关参数初始化：主要是初始化$θ_0, θ_1, ... θ_n$，算法终止距离$ε$和步长$α$

4. 算法过程：
	1. 确定当前位置的损失函数的梯度，对于$θ_i$，其梯度表达式如下：
			$$\frac{∂}{∂θi}J(θ_0,θ_1...,θ_n)$$
	2. 用步长乘以损失函数的梯度，得到当前位置下降的距离，即$α\frac{∂}{∂θi}J(θ_0,θ_1...,θ_n)$；
	3. 确定是否所有的$θ_i$，梯度下降的距离都小于$ε$，如果小于$ε$则算法终止，当前所有的$θ_i(i=0,1,...n)$即为最终结果。否则进入步骤4；
	4. 更新所有的$θ$， 对于$θ_i$，更新表达式如下。更新完毕后继续转入步骤1。
	$$θ_i = θ_i - α \frac{∂}{∂θi}J(θ_0,θ_1...,θ_n)$$
## 2.4 矩阵法

首先，对于输入矩阵$X$为$m * n$的矩阵：
$$ X =
\begin{bmatrix} 
x_{11} & x_{12} & {\cdots}& x_{1n} \\ 
x_{21} & x_{12} & {\cdots}& x_{2n} \\ 
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
x_{m1} & x_{m2} & {\cdots}& x_{mn} \\ 
\end{bmatrix}
=
\begin{bmatrix} 
x_1^\mathrm T \\
x_2^\mathrm T \\
{\cdots} \\
x_m^\mathrm T \\
\end{bmatrix}
$$
所以预测值为$\hat y = Xw$：
$$ X 
=
\begin{bmatrix} 
x_1^\mathrm T \\
x_2^\mathrm T \\
{\cdots} \\
x_m^\mathrm T \\
\end{bmatrix}w 
= 
\begin{bmatrix} 
x_1^\mathrm T w\\
x_2^\mathrm T w\\
{\cdots} \\
x_m^\mathrm T w\\
\end{bmatrix}
=
\begin{bmatrix} 
z_w(x_1)\\
z_w(x_2)\\
{\cdots} \\
z_w(x_m)\\
\end{bmatrix}
$$

	因此预测值与真实值之间的均方误差为:

$$MSE = \frac{1}{2} *(\hat y - y)^2 = \frac{1}{2} *(Xw - y)^2$$

化简：
$$MSE =\frac{1}{2} (Xw - y)^T(Xw - y)$$
$$(Xw - y)^T(Xw - y) = w^TX^TXw - w^TX^Ty-y^TXw+y^Ty$$
$$(Xw - y)^T(Xw - y) = w^TX^TXw - 2y^TXw+y^Ty$$
求导：
$$
\frac {\partial (w^TX^TXw - 2y^TXw+y^Ty)}{\partial w} 
$$
$$ = \frac{\partial {(w^TX^TXw)}} {\partial w} -2\frac{\partial {(y^TXw)}} {\partial w} + \frac{\partial {(y^Ty)}} {\partial w}$$
对于**第一项**：（利用公式化简）
$$ \frac{\partial {(w^TX^TXw)}} {\partial w} = 2(X^TX)w$$
对于**第二项**：（利用公式化简）
$$\frac{\partial {(y^TXw)}} {\partial w} = X^T y$$
对于**第三项**：等于0
因此可以化简为：
$$
\frac {\partial (w^TX^TXw - 2y^TXw+y^Ty)}{\partial w} =
2(X^TX)w - 2X^T y = X^T(Xw - y)
$$
于是：
$$w = w - \alpha X^T(Xw - y)$$
弄清楚向量化是怎么回事就很简单！

# 3. 梯度下降的类型（BGD，SGD，MBGD）

## 3.1 批量梯度下降（Batch Gradient Descent）

批量梯度下降 (BGD) 用于查找训练集（training set）中每个点的误差，并在评估所有训练示例后更新模型。这个过程被称为训练时期（the training epoch）。简而言之，需要在更新参数时使用所有的样本来进行更新。

	$$θ_i = θ_i - α \sum^m_{j=1}(h_θ(x_0^{(j)}, x_1^{(j)}, ... x_n^{(j)}) - y_j)x_i^{(j)}$$
优点：
- 与其他梯度下降法相比，它产生的噪声更少。
- 它产生稳定的梯度下降收敛。
- 它在计算上是高效的，因为所有资源都用于所有训练样本。

## 3.2 随机梯度下降（Stochastic Gradient Descent）

随机梯度下降 (SGD) 是一种梯度下降，每次迭代运行一个训练样本。换句话说，仅仅选取一个样本j来求梯度。
> <small>它为数据集中的每个示例处理一个训练时期，并一次更新每个训练示例的参数。由于它一次只需要一个训练示例，因此更容易存储在分配的内存中。然而，与批量梯度系统相比，它显示出一些计算效率损失，因为它显示需要更多细节和速度的频繁更新。此外，由于频繁更新，它也被视为有噪声的梯度。但是，有时它有助于找到全局最小值并避开局部最小值。</small>

	$$θ_i = θ_i - α (h_θ(x_0^{(j)}, x_1^{(j)}, ... x_n^{(j)}) - y_j)x_i^{(j)}$$

优点：
- 在所需内存中分配更容易。
- 它比批量梯度下降计算速度相对快。
- 它对大型数据集更有效。

## 3.3 小批量梯度下降（Mini-batch Gradient Descent）

Mini Batch 梯度下降是 batch 梯度下降和随机梯度下降的结合。它将训练数据集分成小批量，然后分别对这些批量执行更新。将训练数据集拆分成更小的批次可以在保持批量梯度下降的计算效率和随机梯度下降的速度之间取得平衡。因此，我们可以实现一种特殊类型的梯度下降，具有更高的计算效率和更少的噪声梯度下降。
	$$θ_i = θ_i - α \sum^{t+x-1}_{j=t}(h_θ(x_0^{(j)}, x_1^{(j)}, ... x_n^{(j)}) - y_j)x_i^{(j)}$$
优点：
-   更容易适应分配的内存。
-   它在计算上是高效的。
-   它产生稳定的梯度下降收敛。



# 参考文献

1. [Beginner: Cost Function and Gradient Descent](https://towardsdatascience.com/machine-leaning-cost-function-and-gradient-descend-75821535b2ef)
2. [梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)
3. [Gradient Descent in Machine Learning](https://www.javatpoint.com/gradient-descent-in-machine-learning)
4. [Gradient Descent in Python](https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f)
5. [机器学习 | 优化——梯度下降（矩阵方式描述）](https://www.jianshu.com/p/af118278955e)